{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QuerryEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "documents = SimpleWebPageReader(html_to_text=True).load_data([\"https://llamaindexxx.readthedocs.io/en/latest/understanding/putting_it_all_together/chatbots/building_a_chatbot.html\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "Settings.llm = Ollama(model = 'granite3.2:2b')\n",
    "Settings.embed_model = OllamaEmbedding('nomic-embed-text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SummaryIndex\n",
    "Index = SummaryIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Querry_engine = Index.as_query_engine(streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A chatbot operates as an intelligent conversational tool that interacts with users through text or voice interfaces. These digital entities are widely adopted across industries including customer service, data gathering, personal assistance, and more. Leveraging sophisticated algorithms, they produce contextually fitting responses based on user inputs, thereby amplifying communication effectiveness in diverse scenarios. Essentially, chatbots function as liaisons connecting humans with technology, ensuring harmonious connections in numerous domains.\n"
     ]
    }
   ],
   "source": [
    "print(Querry_engine.query(\"what is a chatbot?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ipywidgets --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided text outlines the process to enhance a \"10-K Chatbot\" that compares risk factors across years using both vector index methods and a sub-question engine, tailored for Uber's annual reports (10-K). Here are key steps to achieve this:\n",
      "\n",
      "1. **Data Preparation**:\n",
      "   Ensure you have the relevant UBER 10-K files in an appropriate format for parsing into JSON. For instance, use LlamaHub with Unstructured Reader for this purpose.\n",
      "   ```python\n",
      "   !pip install llama-hub unstructured\n",
      "\n",
      "   from llama_index.readers.file import UnstructuredReader\n",
      "\n",
      "   years = [2022, 2021, 2020, 2019]\n",
      "\n",
      "   loader = UnstructuredReader()\n",
      "   doc_set = {}\n",
      "   all_docs = []\n",
      "\n",
      "   for year in years:\n",
      "       year_docs = loader.load_data(f\"https://www.dropbox.com/s/{year}/UBER.zip?dl=1\")  # Adapt this URL to the actual download location\n",
      "       all_docs.extend(year_docs)\n",
      "   ```\n",
      "\n",
      "2. **Vector Index Creation**:\n",
      "   Develop a function to construct vector index data structures (e.g., using Gensim). Hereâ€™s an example with Gensim:\n",
      "\n",
      "   ```python\n",
      "   from gensim.models import Doc2Vec, TfidfModel\n",
      "\n",
      "   def create_vector_index(docs):\n",
      "       model = Doc2Vec(min_count=1)  # Adjust min_count as needed\n",
      "       model.build_vocab(docs)\n",
      "       tfidf = TfidfModel(model[docs])\n",
      "\n",
      "       return {\n",
      "           vector: document for document, vector in zip(docs, [tfidf[doc] for doc in docs])\n",
      "       }\n",
      "   ```\n",
      "\n",
      "3. **Querying with Sub-Question Engine**:\n",
      "   Integrate a sub-question engine to manage complex queries concerning multiple years' risk factors:\n",
      "\n",
      "   ```python\n",
      "   import openai\n",
      "\n",
      "   def fetch_risk_factors(user_query):\n",
      "       year_subqueries = [\n",
      "           f\"What are the risk factors described in the {year} SEC 10-K for Uber?\"\n",
      "           for year in range(2020, 2023)\n",
      "       ]\n",
      "\n",
      "       sub_question_results = [\n",
      "           openai.Completion.create(\n",
      "               model=\"text-davinci-003\",\n",
      "               prompt=f\"{year}_query\",\n",
      "               max_tokens=150,\n",
      "               temperature=0.7,\n",
      "               top_p=1,\n",
      "               frequency_penalty=0,\n",
      "               presence_penalty=0\n",
      "           ) for year in range(2020, 2023)\n",
      "       ]\n",
      "\n",
      "       risk_factors = []\n",
      "\n",
      "       for subquery, result in zip(year_subqueries, sub_question_results):\n",
      "           risk_factors.append({\n",
      "               \"role\": \"system\",\n",
      "               \"content\": f\"The {result['choices'][0]['text']}\"  # Extract the first choice (likely correct) from each sub-query response\n",
      "           })\n",
      "\n",
      "       return {\n",
      "           \"role\": \"user\",\n",
      "           \"data\": {\n",
      "               \"risk_factors\": risk_factors,\n",
      "               \"cross_year_query\": user_query\n",
      "           }\n",
      "       }\n",
      "   ```\n",
      "\n",
      "4. **Chatbot Functionality**:\n",
      "   Combine these components into your chatbot function:\n",
      "\n",
      "   ```python\n",
      "   def fetch_10k_info(user_input):\n",
      "       doc_id = get_doc_id_from_query(user_input)  # Replace with actual implementation to map queries to documents\n",
      "\n",
      "       if doc_id in all_docs:\n",
      "           return {\n",
      "               \"role\": \"system\",\n",
      "               \"content\": f\"Here's information about the '{user_input}' based on your UBER 10-K filing:\\n\\n{all_docs[doc_id]['content']}\"\n",
      "           }\n",
      "       else:\n",
      "           return fetch_risk_factors(user_input)\n",
      "\n",
      "   user_input = \"Compare/contrast the risk factors described in the Uber 10-K across years.\"\n",
      "   response = fetch_10k_info(user_input)\n",
      "   print(response[\"content\"])\n",
      "   ```\n",
      "\n",
      "This approach enables your chatbot to offer insights from individual year documents and cross-year comparisons of common risk factors by utilizing vector indexing and the sub-question engine. Further refinement could focus on improving NLP models for more nuanced query understanding or incorporating advanced query handling techniques."
     ]
    }
   ],
   "source": [
    "for chunk in Querry_engine.query(\" What is that text contain?\").response_gen:\n",
    "    print(chunk ,end=\"\",flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "memory = ChatMemoryBuffer.from_defaults(token_limit=1500)\n",
    "\n",
    "chat_engine = Index.as_chat_engine(\n",
    "    chat_mode=\"context\",\n",
    "    memory=memory,\n",
    "    system_prompt=(\n",
    "        \"You are a chatbot, able to have normal interactions, as well as talk\"\n",
    "        \" about an essay discussing Paul Grahams life.\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat_engine.chat(\"Hello!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat_engine.chat(\"What did Paul Graham do growing up?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat_engine.chat(\"Can you tell me more?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
