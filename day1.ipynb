{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%pip install llama-index-llms-ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install streamlit llama-index-llms-ollama llama-index-readers-file llama-index-readers-web"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day1 Using ollama model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paul Graham is a well-known American entrepreneur, programmer, and investor. He is the co-founder of Y Combinator, a popular startup accelerator program that has invested in many successful companies such as Airbnb, Reddit, and Stripe.\n",
      "\n",
      "Graham was born in 1964 and grew up in New Hampshire. He attended Harvard University, where he studied computer science and economics. After graduating from Harvard, Graham moved to California and worked at various tech companies, including Omidyar Network and the Stanford Artificial Intelligence Laboratory (SAIL).\n",
      "\n",
      "In 2005, Graham co-founded Y Combinator with his brother Robert, Kyle, and Jeff Chan. The first class of Y Combinator startups included companies such as Zappos, StumbleUpon, and Spiceworks. Since then, Y Combinator has become one of the most successful startup accelerators in the world, having invested in over 2,000 companies.\n",
      "\n",
      "Graham is known for his straightforward and often contrarian approach to investing in startups. He is a strong believer in the importance of working hard, being open-minded, and taking risks when it comes to entrepreneurship. He has also written extensively on topics such as startup success, entrepreneurship, and the future of work.\n",
      "\n",
      "In addition to his work with Y Combinator, Graham has also been involved in various other initiatives, including the Paul Graham School of Entrepreneurship at Harvard University, which he co-founded in 2015. The school offers courses and programs for entrepreneurs, students, and anyone interested in learning more about entrepreneurship and startup success.\n",
      "\n",
      "Graham is also known for his blunt and sometimes humorous advice on entrepreneurship and investing. He has been featured in various media outlets, including The New York Times, Forbes, and Wired, and has spoken at numerous conferences and events.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "llm = Ollama(model=\"llama3.2\", request_timeout=120.0)\n",
    "resp = llm.complete(\"Who is Paul Graham?\")\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: Me hearty! Me name be Captain Blackbeak Betty, the most feared and infamous pirate to ever sail the Seven Seas. Me reputation precedes me, and me name strikes terror into the hearts o' all who hear it. But don't ye worry, I be a pirate with a heart o' gold... and a penchant for tellin' tales and drinkin' grog! *takes a swig from a nearby flask*\n",
      "\n",
      "Now, what brings ye to these fair waters? Are ye lookin' to join me crew and sail the high seas with the bravest pirate on the ocean? Or perhaps ye be wantin' to learn the secrets o' the sea from yours truly? Whatever yer reason, I be willin' to listen... but don't think about tryin' to steal me treasure, or ye'll be walkin' the plank!\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=\"system\", content=\"You are a pirate with a colorful personality\"\n",
    "    ),\n",
    "    ChatMessage(role=\"user\", content=\"What is your name\"),\n",
    "]\n",
    "resp = llm.chat(messages)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.stream_complete(\"Who is Paul Graham?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paul Graham is a well-known entrepreneur, investor, and computer programmer. He was born in 1964 in Oakland, California.\n",
      "\n",
      "Graham co-founded several successful software companies, including:\n",
      "\n",
      "1. Viaweb (now Shopify): In 1996, he co-founded Viaweb, which provided an online platform for creating e-commerce websites. The company gained popularity, especially among small businesses and individuals, and eventually sold to the Canadian company NetSuite in 2009.\n",
      "2. Userland: Graham also founded Userland, a company that developed various web applications, including the popular \"wiki\" site, Zim Wiki.\n",
      "\n",
      "Graham is particularly known for his work at Y Combinator, a venture capital firm he co-founded with Jeff Clavier and Robert Musoch in 2005. Y Combinator focuses on investing in startups and providing them with resources, mentorship, and networking opportunities to help them grow and succeed.\n",
      "\n",
      "Paul Graham has been an influential figure in the startup ecosystem, particularly among entrepreneurs and investors in Silicon Valley. He has written extensively about entrepreneurship, investing, and technology trends, often sharing his insights through articles, talks, and lectures at conferences like Y Combinator's annual Demo Day events.\n",
      "\n",
      "Graham is also known for his unique approach to mentoring and supporting startups. He has a strict and no-nonsense attitude, which can be intimidating to some entrepreneurs, but also helps individuals who are driven to succeed."
     ]
    }
   ],
   "source": [
    "for r in response:\n",
    "    print(r.delta, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using stream_chat endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrrr, me hearty! Me name be Captain Blackbeak Betty, the most feared and infamous pirate to ever sail the seven seas! *winks* Me and me trusty crew o' scurvy dogs have been plunderin' and pillagin' for nigh on 20 years, and we've got a reputation for bein' the most cunning and ruthless buccaneers on the high seas!\n",
      "\n",
      "Me ship, the \"Black Swan\", be me pride and joy. She's fast, she's deadly, and she's got more secrets than a chest overflowin' with golden doubloons! *chuckles*\n",
      "\n",
      "Now, what be bringin' ye to these waters? Are ye lookin' to join me crew and sail the seas with the infamous Captain Blackbeak Betty? Or maybe ye just want to hear tales o' adventure and bravery on the high seas? Whatever yer reason, I be willin' to listen... for a price, o' course! *winks*"
     ]
    }
   ],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=\"system\", content=\"You are a pirate with a colorful personality\"\n",
    "    ),\n",
    "    ChatMessage(role=\"user\", content=\"What is your name\"),\n",
    "]\n",
    "resp = llm.stream_chat(messages)\n",
    "for r in resp:\n",
    "    print(r.delta, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JSON Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"name\": \"Paul Graham\",\n",
      "    \"title\": \"\",\n",
      "    \"occupation\": \"Entrepreneur, Investor, and Philosopher\",\n",
      "    \"nationality\": \"American\",\n",
      "    \"birth_date\": \"1950-02-16\",\n",
      "    \"death_date\": null,\n",
      "    \"known_for\": [\n",
      "        \"Founding of Y Combinator\",\n",
      "        \"Support for Startup Founders\",\n",
      "        \"Philosophy on Entrepreneurship\"\n",
      "    ],\n",
      "    \"influential_works\": [],\n",
      "    \"education\": \"\",\n",
      "    \"awards\": []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "llm = Ollama(model=\"llama3.2:latest\", request_timeout=120.0, json_mode=True)\n",
    "response = llm.complete(\n",
    "    \"Who is Paul Graham? Output as a structured JSON object.\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"name\":\"Pumped Up Kicks\",\"artist\":\"Foster the People\"}\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.bridge.pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Song(BaseModel):\n",
    "    \"\"\"A song with name and artist.\"\"\"\n",
    "\n",
    "    name: str\n",
    "    artist: str\n",
    "\n",
    "llm = Ollama(model=\"llama3.2:latest\", request_timeout=120.0)\n",
    "\n",
    "sllm = llm.as_structured_llm(Song)\n",
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "response = sllm.chat([ChatMessage(role=\"user\", content=\"Name a random song!\")])\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# with async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"name\":\"Mr. Brightside\",\"artist\":\"The Killers\"}\n"
     ]
    }
   ],
   "source": [
    "response = await sllm.achat(\n",
    "    [ChatMessage(role=\"user\", content=\"Name a random song!\")]\n",
    ")\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"name\":null,\"artist\":null}\n",
      "{\"name\":null,\"artist\":null}\n",
      "{\"name\":null,\"artist\":null}\n",
      "{\"name\":\"\",\"artist\":null}\n",
      "{\"name\":\"Ever\",\"artist\":null}\n",
      "{\"name\":\"Everlong\",\"artist\":null}\n",
      "{\"name\":\"Everlong\",\"artist\":null}\n",
      "{\"name\":\"Everlong\",\"artist\":null}\n",
      "{\"name\":\"Everlong\",\"artist\":null}\n",
      "{\"name\":\"Everlong\",\"artist\":null}\n",
      "{\"name\":\"Everlong\",\"artist\":\"\"}\n",
      "{\"name\":\"Everlong\",\"artist\":\"Foo\"}\n",
      "{\"name\":\"Everlong\",\"artist\":\"Foo Fighters\"}\n",
      "{\"name\":\"Everlong\",\"artist\":\"Foo Fighters\"}\n",
      "{\"name\":\"Everlong\",\"artist\":\"Foo Fighters\"}\n"
     ]
    }
   ],
   "source": [
    "response_gen = sllm.stream_chat(\n",
    "    [ChatMessage(role=\"user\", content=\"Name a random song!\")]\n",
    ")\n",
    "for r in response_gen:\n",
    "    print(r.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Modal Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: It's a promotional graphic. It shows the title \"OLLAMA WEB UI\" which may refer to an application or service related to web user interface design, possibly named after Olloama. The man on the right could be associated with OLLAMA WEB UI as it seems he represents them or is endorsing their product/service.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.llms import ChatMessage, TextBlock, ImageBlock\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "llm = Ollama(model=\"minicpm-v\", request_timeout=120.0)\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=\"user\",\n",
    "        blocks=[\n",
    "            TextBlock(text=\"What is this?\"),\n",
    "            ImageBlock(path=\"image.png\"),\n",
    "        ],\n",
    "    ),\n",
    "]\n",
    "\n",
    "resp = llm.chat(messages)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pydub import AudioSegment\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "def image_to_sound(image_path, output_mp3_path, duration=5, sample_rate=44100):\n",
    "    \"\"\"\n",
    "    Convert an image to a sound and save it as an MP3 file.\n",
    "\n",
    "    Parameters:\n",
    "    - image_path: str, path to the input image file (e.g., 'image.jpg').\n",
    "    - output_mp3_path: str, path to save the output MP3 file (e.g., 'output.mp3').\n",
    "    - duration: float, desired duration of the audio in seconds (default=5).\n",
    "    - sample_rate: int, sample rate of the audio in Hz (default=44100).\n",
    "    \n",
    "    Raises:\n",
    "    - ValueError: If the image has no pixels or the duration is too short.\n",
    "    - FileNotFoundError: If the image file cannot be found.\n",
    "    \"\"\"\n",
    "    # Load the image and convert it to grayscale\n",
    "    img = Image.open(image_path).convert('L')\n",
    "    pixels = np.array(img).flatten()\n",
    "    N = len(pixels)\n",
    "    \n",
    "    if N == 0:\n",
    "        raise ValueError(\"Image has no pixels\")\n",
    "    \n",
    "    # Interpolate pixel values to match the desired audio duration\n",
    "    M = int(duration * sample_rate)\n",
    "    if M == 0:\n",
    "        raise ValueError(\"Duration too short\")\n",
    "    \n",
    "    x = np.linspace(0, 1, N)\n",
    "    f = interp1d(x, pixels, kind='linear')\n",
    "    x_new = np.linspace(0, 1, M)\n",
    "    samples = f(x_new)\n",
    "    \n",
    "    # Normalize samples to the range [-1, 1]\n",
    "    normalized = (samples / 127.5) - 1\n",
    "    \n",
    "    # Convert to 16-bit audio samples\n",
    "    audio_samples = (normalized * 32767).astype(np.int16)\n",
    "    \n",
    "    # Create raw audio data\n",
    "    sample_bytes = audio_samples.tobytes()\n",
    "    \n",
    "    # Create an audio segment (mono, 16-bit, 44100 Hz by default)\n",
    "    audio = AudioSegment(\n",
    "        data=sample_bytes,\n",
    "        sample_width=2,  # 2 bytes = 16 bits\n",
    "        frame_rate=sample_rate,\n",
    "        channels=1  # Mono audio\n",
    "    )\n",
    "    \n",
    "    # Export the audio as an MP3 file\n",
    "    audio.export(output_mp3_path, format=\"mp3\")\n",
    "    # Generate a 10-second MP3 from an image\n",
    "image_to_sound(\"image.png\", \"output_sound.mp3\", duration=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated audio duration: 10.00 seconds\n",
      "Generated audio duration: 0.29 seconds\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.fft import irfft\n",
    "from pydub import AudioSegment\n",
    "import os\n",
    "\n",
    "def image_to_sound(image_path, output_mp3_path, duration=None, sample_rate=44100, default_samples_per_row=44):\n",
    "    \"\"\"\n",
    "    Convert an image to a sound and save it as an MP3 file, using all three RGB channels.\n",
    "\n",
    "    Parameters:\n",
    "    - image_path: str, path to the input image file (e.g., 'image.jpg').\n",
    "    - output_mp3_path: str, path to save the output MP3 file (e.g., 'output.mp3').\n",
    "    - duration: float or None, desired duration of the audio in seconds (optional).\n",
    "    - sample_rate: int, sample rate of the audio in Hz (default=44100).\n",
    "    - default_samples_per_row: int, samples per row if duration is not specified (default=44).\n",
    "    \n",
    "    Raises:\n",
    "    - ValueError: If the image has no pixels.\n",
    "    - FileNotFoundError: If the image file cannot be found.\n",
    "    \"\"\"\n",
    "    # Check if the image file exists\n",
    "    if not os.path.exists(image_path):\n",
    "        raise FileNotFoundError(f\"Image file not found: {image_path}\")\n",
    "    \n",
    "    # Load the image and convert to RGB\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    img_array = np.array(img)  # Shape: (H, W, 3)\n",
    "    H, W, _ = img_array.shape\n",
    "    \n",
    "    # Validate image dimensions\n",
    "    if H == 0 or W == 0:\n",
    "        raise ValueError(\"Image has no pixels\")\n",
    "    \n",
    "    # Calculate samples per row\n",
    "    if duration is not None:\n",
    "        M = max(2, round(duration * sample_rate / H))\n",
    "    else:\n",
    "        M = default_samples_per_row  # Default to 44 samples per row (~1ms at 44100 Hz)\n",
    "    \n",
    "    K = M // 2 + 1  # Number of frequency bins for IRFFT\n",
    "    \n",
    "    # Divide frequency bins among R, G, B\n",
    "    K1 = K // 3\n",
    "    K2 = K // 3\n",
    "    K3 = K - K1 - K2\n",
    "    \n",
    "    # Precompute interpolation points for resampling\n",
    "    x = np.linspace(0, 1, W)\n",
    "    x_new_R = np.linspace(0, 1, K1)\n",
    "    x_new_G = np.linspace(0, 1, K2)\n",
    "    x_new_B = np.linspace(0, 1, K3)\n",
    "    \n",
    "    audio_segments = []\n",
    "    \n",
    "    # Process each row of the image\n",
    "    for i in range(H):\n",
    "        # Extract and normalize RGB values for row i\n",
    "        R_row = img_array[i, :, 0] / 255.0  # Red channel\n",
    "        G_row = img_array[i, :, 1] / 255.0  # Green channel\n",
    "        B_row = img_array[i, :, 2] / 255.0  # Blue channel\n",
    "        \n",
    "        # Resample each channel to its frequency bins\n",
    "        interp_R = interp1d(x, R_row, kind='linear', bounds_error=False, fill_value=0)\n",
    "        interp_G = interp1d(x, G_row, kind='linear', bounds_error=False, fill_value=0)\n",
    "        interp_B = interp1d(x, B_row, kind='linear', bounds_error=False, fill_value=0)\n",
    "        \n",
    "        R_resampled = interp_R(x_new_R)\n",
    "        G_resampled = interp_G(x_new_G)\n",
    "        B_resampled = interp_B(x_new_B)\n",
    "        \n",
    "        # Construct the frequency spectrum\n",
    "        S = np.zeros(K, dtype=np.float64)\n",
    "        S[0:K1] = R_resampled          # Red frequencies\n",
    "        S[K1:K1 + K2] = G_resampled    # Green frequencies\n",
    "        S[K1 + K2:K] = B_resampled     # Blue frequencies\n",
    "        \n",
    "        # Generate time-domain signal using inverse real FFT\n",
    "        s_i = irfft(S, n=M)\n",
    "        audio_segments.append(s_i)\n",
    "    \n",
    "    # Concatenate all row signals into one waveform\n",
    "    a = np.concatenate(audio_segments)\n",
    "    \n",
    "    # Normalize to prevent clipping\n",
    "    max_amp = np.max(np.abs(a))\n",
    "    if max_amp > 0:\n",
    "        a = a / max_amp * 0.9  # Scale to 90% of maximum amplitude\n",
    "    \n",
    "    # Convert to 16-bit audio samples\n",
    "    audio_samples = (a * 32767).astype(np.int16)\n",
    "    \n",
    "    # Create raw audio data\n",
    "    sample_bytes = audio_samples.tobytes()\n",
    "    \n",
    "    # Create an audio segment (mono, 16-bit, 44100 Hz)\n",
    "    audio = AudioSegment(\n",
    "        data=sample_bytes,\n",
    "        sample_width=2,  # 2 bytes = 16 bits\n",
    "        frame_rate=sample_rate,\n",
    "        channels=1  # Mono audio\n",
    "    )\n",
    "    \n",
    "    # Export the audio as an MP3 file\n",
    "    audio.export(output_mp3_path, format=\"mp3\")\n",
    "    \n",
    "    # Print the actual duration\n",
    "    actual_duration = len(audio_samples) / sample_rate\n",
    "    print(f\"Generated audio duration: {actual_duration:.2f} seconds\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # With duration specified\n",
    "    image_to_sound(\"image.png\", \"output_with_duration.mp3\", duration=10)\n",
    "    \n",
    "    # Without duration (uses default samples per row)\n",
    "    image_to_sound(\"/home/ramachandra/Pictures/Screenshot from 2025-03-07 13-28-46.png\", \"output_no_duration.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
