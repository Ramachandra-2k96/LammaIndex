{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader,VectorStoreIndex,SummaryIndex\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.tools import FunctionTool,QueryEngineTool\n",
    "from llama_index.core.vector_stores import MetadataFilters,FilterCondition\n",
    "from typing import List,Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader('data').load_data()\n",
    "print(len(documents))\n",
    "print(f\"Document Metadata: {documents[0].metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = SentenceSplitter(chunk_size=1024,chunk_overlap=100)\n",
    "nodes = splitter.get_nodes_from_documents(documents)\n",
    "print(f\"Length of nodes : {len(nodes)}\")\n",
    "print(f\"get the content for node 0 :{nodes[0].get_content(metadata_mode='all')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "db = chromadb.PersistentClient(path=\"./chroma_db_mistral\")\n",
    "chroma_collection = db.get_or_create_collection(\"multidocument-agent\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.core import Settings\n",
    "#\n",
    "Settings.embed_model = OllamaEmbedding(model_name=\"nomic-embed-text\")\n",
    "Settings.chunk_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "Settings.llm = Ollama(model=\"granite3.1-dense\",request_timeout=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"BERT_arxiv\"\n",
    "vector_index = VectorStoreIndex(nodes,storage_context=storage_context)\n",
    "vector_index.storage_context.vector_store.persist(persist_path=\"/content/chroma_db\")\n",
    "#\n",
    "# Define Vectorstore Autoretrieval tool\n",
    "def vector_query(query:str,page_numbers:Optional[List[str]]=None)->str:\n",
    "  '''\n",
    "  perform vector search over index on\n",
    "  query(str): query string needs to be embedded\n",
    "  page_numbers(List[str]): list of page numbers to be retrieved,\n",
    "                          leave blank if we want to perform a vector search over all pages\n",
    "  '''\n",
    "  page_numbers = page_numbers or []\n",
    "  metadata_dict = [{\"key\":'page_label',\"value\":p} for p in page_numbers]\n",
    "  #\n",
    "  query_engine = vector_index.as_query_engine(similarity_top_k =4,\n",
    "                                              filters = MetadataFilters.from_dicts(metadata_dict,\n",
    "                                                                                    condition=FilterCondition.OR)\n",
    "                                              )\n",
    "  #\n",
    "  response = query_engine.query(query)\n",
    "  return response\n",
    "#\n",
    "#llamiondex FunctionTool wraps any python function we feed it\n",
    "vector_query_tool = FunctionTool.from_defaults(name=f\"vector_tool_{name}\",\n",
    "                                              fn=vector_query)\n",
    "# Prepare Summary Tool\n",
    "summary_index = SummaryIndex(nodes)\n",
    "summary_query_engine = summary_index.as_query_engine(response_mode=\"tree_summarize\",\n",
    "                                                      se_async=True,)\n",
    "summary_query_tool = QueryEngineTool.from_defaults(name=f\"summary_tool_{name}\",\n",
    "                                                    query_engine=summary_query_engine,\n",
    "                                                  description=(\"Use ONLY IF you want to get a holistic summary of the documents.\"\n",
    "                                              \"DO NOT USE if you have specified questions over the documents.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = Settings.llm.predict_and_call([vector_query_tool],\n",
    "                                \"Summarize the content in page number 2\",\n",
    "                                verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_tools(file_path:str,name:str)->str:\n",
    "  '''\n",
    "  get vector query and sumnmary query tools from a document\n",
    "  '''\n",
    "  #load documents\n",
    "  documents = SimpleDirectoryReader(input_files = [file_path]).load_data()\n",
    "  print(f\"length of nodes\")\n",
    "  splitter = SentenceSplitter(chunk_size=1024,chunk_overlap=100)\n",
    "  nodes = splitter.get_nodes_from_documents(documents)\n",
    "  print(f\"Length of nodes : {len(nodes)}\")\n",
    "  #instantiate Vectorstore\n",
    "  vector_index = VectorStoreIndex(nodes,storage_context=storage_context)\n",
    "  vector_index.storage_context.vector_store.persist(persist_path=\"/content/chroma_db\")\n",
    "  #\n",
    "  # Define Vectorstore Autoretrieval tool\n",
    "  def vector_query(query:str,page_numbers:Optional[List[str]]=None)->str:\n",
    "    '''\n",
    "    perform vector search over index on\n",
    "    query(str): query string needs to be embedded\n",
    "    page_numbers(List[str]): list of page numbers to be retrieved,\n",
    "                            leave blank if we want to perform a vector search over all pages\n",
    "    '''\n",
    "    page_numbers = page_numbers or []\n",
    "    metadata_dict = [{\"key\":'page_label',\"value\":p} for p in page_numbers]\n",
    "    #\n",
    "    query_engine = vector_index.as_query_engine(similarity_top_k =2,\n",
    "                                                filters = MetadataFilters.from_dicts(metadata_dict,\n",
    "                                                                                     condition=FilterCondition.OR)\n",
    "                                                )\n",
    "    #\n",
    "    response = query_engine.query(query)\n",
    "    return response\n",
    "  #\n",
    "  #llamiondex FunctionTool wraps any python function we feed it\n",
    "  vector_query_tool = FunctionTool.from_defaults(name=f\"vector_tool_{name}\",\n",
    "                                                fn=vector_query)\n",
    "  # Prepare Summary Tool\n",
    "  summary_index = SummaryIndex(nodes)\n",
    "  summary_query_engine = summary_index.as_query_engine(response_mode=\"tree_summarize\",\n",
    "                                                       se_async=True,)\n",
    "  summary_query_tool = QueryEngineTool.from_defaults(name=f\"summary_tool_{name}\",\n",
    "                                                     query_engine=summary_query_engine,\n",
    "                                                    description=(\"Use ONLY IF you want to get a holistic summary of the documents.\"\n",
    "                                                \"DO NOT USE if you have specified questions over the documents.\"))\n",
    "  return vector_query_tool,summary_query_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "root_path = \"data\"\n",
    "file_name = []\n",
    "file_path = []\n",
    "for files in os.listdir(root_path):\n",
    "  if files.endswith(\".pdf\"):\n",
    "    file_name.append(files.split(\".\")[0])\n",
    "    file_path.append(os.path.join(root_path,files))\n",
    "#\n",
    "print(file_name)\n",
    "print(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_to_tools_dict = {}\n",
    "for name,filename in zip(file_name,file_path):\n",
    "  vector_query_tool,summary_query_tool = get_doc_tools(filename,name)\n",
    "  papers_to_tools_dict[name] = [vector_query_tool,summary_query_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_tools = [t for f in file_name for t in papers_to_tools_dict[f]]\n",
    "initial_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.objects import ObjectIndex\n",
    "#\n",
    "obj_index = ObjectIndex.from_objects(initial_tools,index_cls=VectorStoreIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_retriever = obj_index.as_retriever(similarity_top_k=2)\n",
    "tools = obj_retriever.retrieve(\"compare and contrast the papers self rag and corrective rag\")\n",
    "#\n",
    "print(tools[0].metadata)\n",
    "print(tools[1].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "#\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(tool_retriever=obj_retriever,\n",
    "                                                     llm=Settings.llm,\n",
    "                                                     system_prompt=\"\"\"You are an agent designed to answer queries over a set of given papers.\n",
    "                                                     Please always use the tools provided to answer a question.Do not rely on prior knowledge.\"\"\",\n",
    "                                                     verbose=True)\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent.query(\"Compare and contrast self rag and crag.\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent.query(\"Summarize the IonIdea company.\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent.query(\"What was the first thing i told to you?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.core import Settings, SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "from llama_index.core.chat_engine import CondensePlusContextChatEngine\n",
    "\n",
    "llm = Ollama(model=\"llama3.2\") \n",
    "embed_model = OllamaEmbedding(model_name=\"nomic-embed-text\")\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "documents = SimpleDirectoryReader(input_dir=\"data\", required_exts=[\".pdf\"]).load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "retriever = index.as_retriever()\n",
    "memory = ChatMemoryBuffer.from_defaults(token_limit=4500)\n",
    "\n",
    "chat_engine = CondensePlusContextChatEngine.from_defaults(\n",
    "    retriever=retriever, \n",
    "    memory=memory, \n",
    "    llm=llm,\n",
    "    system_prompt=(\n",
    "        \"You are a retrieval-augmented AI assistant. Your task is to decide whether a query requires document retrieval. \"\n",
    "        \"If the user asks about general knowledge, answer directly without retrieving documents. \"\n",
    "        \"If the query is about specific documents, reports, or evidence, retrieve relevant documents and provide citations. \"\n",
    "        \"At the end of your response, always indicate whether document retrieval was used. \"\n",
    "        \"Respond in a clear, informative, and professional manner.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "def chat_with_proof(query):\n",
    "    print(f\"\\nðŸ”¹ User: {query}\")\n",
    "    response_stream = chat_engine.stream_chat(query)\n",
    "\n",
    "    print(\"\\nðŸ”¹ AI Response:\")\n",
    "    for chunk in response_stream.response_gen:\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "\n",
    "    print(\"\\n\")  \n",
    "\n",
    "# 6. Run Example Queries\n",
    "chat_with_proof(\"Summarize the attached PDFs.\") \n",
    "chat_with_proof(\"What are the key insights from the documents?\") \n",
    "chat_with_proof(\"Explain Quantum Computing.\") \n",
    "chat_with_proof(\"How does this relate to my previous question?\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.core import Settings, SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "from llama_index.core.chat_engine import CondensePlusContextChatEngine\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "import re\n",
    "\n",
    "# 1. Initialize AI Model & Embeddings\n",
    "llm = Ollama(model=\"llama3.2\")\n",
    "embed_model = OllamaEmbedding(model_name=\"nomic-embed-text\")\n",
    "\n",
    "# 2. Set Global AI Settings\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "# 3. Load and Index PDFs with proper metadata extraction\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_dir=\"data\", \n",
    "    required_exts=[\".pdf\"],\n",
    "    filename_as_id=True,  # Use filename as document ID\n",
    "    recursive=True       # Include subdirectories if any\n",
    ").load_data()\n",
    "\n",
    "# Print document metadata to verify it's being captured correctly\n",
    "print(f\"Loaded {len(documents)} documents\")\n",
    "for doc in documents[:2]:  # Print first 2 docs as a sample\n",
    "    print(f\"Document: {doc.metadata.get('file_name', 'No filename')} | \"\n",
    "          f\"Pages: {doc.metadata.get('page_label', 'Unknown')}\")\n",
    "\n",
    "# 4. Fixed text parsing with compatible parameters\n",
    "node_parser = SentenceSplitter(\n",
    "    chunk_size=1024,\n",
    "    chunk_overlap=200,\n",
    "    separator=\" \",       # Space as separator (helps with PDF text)\n",
    "    paragraph_separator=\"\\n\\n\"\n",
    ")\n",
    "\n",
    "# 5. Create the index with better metadata handling\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    node_parser=node_parser,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "# Configure retriever to include more context for better answers\n",
    "retriever = index.as_retriever(\n",
    "    similarity_top_k=3\n",
    ")\n",
    "\n",
    "# 6. Create Memory-Enabled Chat Engine with optimized prompt\n",
    "memory = ChatMemoryBuffer.from_defaults(token_limit=4500)\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are an AI assistant that helps users understand document content. Follow these rules strictly:\n",
    "\n",
    "1. FIRST STEP: Determine if the query is about the document content:\n",
    "   - If YES: Use document retrieval to find relevant information\n",
    "   - If NO: Politely explain you're configured to only answer questions about the provided documents\n",
    "\n",
    "2. When using document retrieval:\n",
    "   - Cite sources clearly with the format [Source: {file_name}, page {page_label}]\n",
    "   - Include document references only when directly quoting or paraphrasing\n",
    "   - End your response with a \"Sources:\" section listing all documents referenced\n",
    "\n",
    "3. Your response format:\n",
    "   - Begin with \"Retrieval used: Yes\" or \"Retrieval used: No\" as appropriate\n",
    "   - Be clear, concise and professional\n",
    "   - Use multi-step reasoning when answering complex questions\n",
    "\n",
    "4. You must REFUSE to answer questions unrelated to the documents, even if you know the answer.\n",
    "\n",
    "NOTE: Always check if the metadata contains file_name and page_label before using them in citations.\n",
    "\"\"\"\n",
    "\n",
    "chat_engine = CondensePlusContextChatEngine.from_defaults(\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    llm=llm,\n",
    "    system_prompt=system_prompt,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 7. Function to clean up garbled text for display\n",
    "def clean_text(text):\n",
    "    # Remove excessive whitespace\n",
    "    cleaned = re.sub(r'\\s+', ' ', text)\n",
    "    # Fix common OCR errors (like \"t e x t\" â†’ \"text\")\n",
    "    cleaned = re.sub(r'(\\w) (\\w) (\\w)', r'\\1\\2\\3', cleaned)\n",
    "    cleaned = re.sub(r'(\\w) (\\w)', r'\\1\\2', cleaned)\n",
    "    return cleaned.strip()\n",
    "\n",
    "# 8. Improved streaming chat function with better metadata display\n",
    "def chat_with_proof(query):\n",
    "    print(f\"\\nUser: {query}\")\n",
    "    \n",
    "    # Get the AI response using streaming mode\n",
    "    response_stream = chat_engine.stream_chat(query)\n",
    "    \n",
    "    print(\"\\nAI Response:\")\n",
    "    for chunk in response_stream.response_gen:\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Enhanced reference display with proper metadata\n",
    "    if response_stream.source_nodes:\n",
    "        print(\"Sources Used:\")\n",
    "        for i, node in enumerate(response_stream.source_nodes, start=1):\n",
    "            metadata = node.metadata or {}\n",
    "            \n",
    "            # Extract proper metadata\n",
    "            filename = metadata.get(\"file_name\", \n",
    "                     metadata.get(\"filename\", \n",
    "                     metadata.get(\"source\", \"Unknown Document\")))\n",
    "            \n",
    "            page = metadata.get(\"page_label\", \n",
    "                  metadata.get(\"page_number\", \n",
    "                  metadata.get(\"page\", \"Unknown Page\")))\n",
    "            \n",
    "            # Clean up the text excerpt\n",
    "            excerpt = clean_text(node.text[:300])\n",
    "            \n",
    "            print(f\"Source {i}: {filename} (Page {page})\")\n",
    "            print(f\"Excerpt: {excerpt}...\")\n",
    "            print(\"-\" * 50)\n",
    "    else:\n",
    "        print(\"Retrieval used: No\")\n",
    "\n",
    "# 9. Interactive session\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Document Q&A System Initialized. Type 'exit' to quit.\")\n",
    "    print(\"This system is configured to answer questions about the documents in the 'data' folder.\")\n",
    "    \n",
    "    while True:\n",
    "        user_query = input(\"\\nYour question: \")\n",
    "        if user_query.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        chat_with_proof(user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.core import Settings, SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "from llama_index.core.chat_engine import CondensePlusContextChatEngine\n",
    "from llama_index.core.extractors import SummaryExtractor, QuestionsAnsweredExtractor, TitleExtractor, KeywordExtractor\n",
    "import chromadb\n",
    "import os\n",
    "import re\n",
    "import uuid\n",
    "\n",
    "# 1. Initialize AI Model & Embeddings\n",
    "llm = Ollama(model=\"llama3.2\")\n",
    "embed_model = OllamaEmbedding(model_name=\"nomic-embed-text\")\n",
    "\n",
    "# 2. Set Global AI Settings\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "# 3. Initialize ChromaDB Client\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_db_IonIdea\")\n",
    "collection_name = \"document_knowledge_base\"\n",
    "\n",
    "collection = chroma_client.get_or_create_collection(collection_name)\n",
    "\n",
    "# 4. Initialize Chat Memory with Token Limit\n",
    "chat_memory = ChatMemoryBuffer(token_limit=2048)\n",
    "\n",
    "# 5. Load and Process Documents (Only if ChromaDB is empty)\n",
    "if collection.count() == 0:\n",
    "    print(\"No existing collection found. Extracting knowledge...\")\n",
    "    \n",
    "    documents = SimpleDirectoryReader(\n",
    "        input_dir=\"data\", \n",
    "        required_exts=[\".pdf\"],\n",
    "        filename_as_id=True,\n",
    "        recursive=True\n",
    "    ).load_data()\n",
    "    \n",
    "    print(f\"Loaded {len(documents)} documents\")\n",
    "    \n",
    "    # 6. Extract structured knowledge (Summaries + Q&A + Knowledge Graph)\n",
    "    node_parser = SentenceSplitter(chunk_size=1024, chunk_overlap=200, separator=\" \", paragraph_separator=\"\\n\\n\")\n",
    "    extractors = [\n",
    "        TitleExtractor(nodes=5),\n",
    "        QuestionsAnsweredExtractor(questions=3),\n",
    "        SummaryExtractor(summaries=[\"self\"]),\n",
    "        KeywordExtractor(),\n",
    "    ]\n",
    "    index = VectorStoreIndex.from_documents(documents, node_parser=node_parser, extractors=extractors, show_progress=True)\n",
    "    \n",
    "    retriever = index.as_retriever(similarity_top_k=3)\n",
    "    \n",
    "    # Store parsed knowledge into ChromaDB\n",
    "    for doc in documents:\n",
    "        metadata = doc.metadata\n",
    "        filename = metadata.get(\"file_name\", \"Unknown File\")\n",
    "        page = metadata.get(\"page_label\", \"Unknown Page\")\n",
    "        content = doc.text\n",
    "        \n",
    "        doc_id = str(uuid.uuid4())  # Generate unique ID\n",
    "        \n",
    "        collection.add(\n",
    "            ids=[doc_id],\n",
    "            documents=[content],\n",
    "            metadatas=[{\"file_name\": filename, \"page_label\": page}]\n",
    "        )\n",
    "        print(f\"Stored: {filename} - Page {page}\")\n",
    "else:\n",
    "    print(\"Existing collection found. Skipping extraction.\")\n",
    "\n",
    "# 7. System Prompt to Allow Casual Conversations but Restrict Answers to Context\n",
    "system_prompt = \"\"\"\n",
    "You are an AI assistant designed to answer document-related queries. Your rules:\n",
    "\n",
    "1. Casual conversations (e.g., greetings) are allowed.\n",
    "2. You strictly refuse to answer any question that is outside the provided documents.\n",
    "3. When retrieving information, cite sources correctly with [Source: {file_name}, Page {page_label}].\n",
    "4. If the query has no relevant document, politely decline to answer.\n",
    "5. You must generate responses based on retrieved information only when absolutely necessary.\n",
    "\"\"\"\n",
    "\n",
    "# 8. Function for retrieving knowledge from ChromaDB\n",
    "def retrieve_knowledge(query):\n",
    "    results = collection.query(query_texts=[query], n_results=3)\n",
    "    \n",
    "    sources = []\n",
    "    for doc, meta_list in zip(results[\"documents\"], results[\"metadatas\"]):\n",
    "        if not meta_list:  # Ensure there is metadata\n",
    "            continue\n",
    "        meta = meta_list[0]  # Extract the first metadata dictionary\n",
    "        filename = meta.get(\"file_name\", \"Unknown File\")\n",
    "        page = meta.get(\"page_label\", \"Unknown Page\")\n",
    "        sources.append((filename, page, doc[0]))  # doc[0] since documents are also lists\n",
    "    \n",
    "    return sources\n",
    "\n",
    "# 9. Function to clean retrieved text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove excessive whitespace\n",
    "    return text.strip()\n",
    "\n",
    "# 10. Chat Function with Proper Citations\n",
    "def chat_with_proof(query):\n",
    "    print(f\"\\nUser: {query}\")\n",
    "    \n",
    "    sources = retrieve_knowledge(query)\n",
    "    \n",
    "    if not sources:\n",
    "        print(\"AI: I'm sorry, but I can only answer questions related to the provided documents.\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nAI Response:\")\n",
    "    \n",
    "    response_context = \"\"\n",
    "    for filename, page, excerpt in sources:\n",
    "        excerpt = clean_text(excerpt[:300])\n",
    "        response_context += f\"{excerpt}\\n\"\n",
    "    \n",
    "    # AI Generates a response based on retrieved knowledge if necessary\n",
    "    if response_context:\n",
    "        chat_memory.put({\"query\": query, \"response\": response_context})\n",
    "        generated_response = llm.complete(response_context + \"\\n\\nBased on this, answer the user's query: \" + query)\n",
    "        print(generated_response.text)\n",
    "    else:\n",
    "        print(\"AI: I cannot answer this question as it is outside the scope of the provided documents.\")\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# 11. Interactive Session\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Document Q&A System Initialized. Type 'exit' to quit.\")\n",
    "    while True:\n",
    "        user_query = input(\"\\nYour question: \")\n",
    "        if user_query.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        chat_with_proof(user_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing collection found. Extracting knowledge...\n",
      "Loaded 6 documents\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cc594d3843b422980ff66037e8cf28f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "474ef9a079eb47ea84c0f09e996c9632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored: Leading the Charge The Future of Healthcare Software Solutions.pdf - Page 1\n",
      "Stored: Leading the Charge The Future of Healthcare Software Solutions.pdf - Page 2\n",
      "Stored: Leading the Charge The Future of Healthcare Software Solutions.pdf - Page 3\n",
      "Stored: Navigating the Future Pioneering Trends in Manufacturing Software Development.pdf - Page 1\n",
      "Stored: Navigating the Future Pioneering Trends in Manufacturing Software Development.pdf - Page 2\n",
      "Stored: Navigating the Future Pioneering Trends in Manufacturing Software Development.pdf - Page 3\n",
      "Document Q&A System Initialized. Type 'exit' to quit.\n",
      "\n",
      "User: hello\n",
      "\n",
      "AI Response:\n",
      "Hello! It seems like you're interested in learning more about IonIde or HealthCareSoft. However, without more context, it's difficult to provide specific information.\n",
      "\n",
      "Healthcare software can encompass a broad range of tools and technologies designed for healthcare providers, patients, and other stakeholders. These systems are often used for managing medical records, billing, scheduling appointments, and much more.\n",
      "\n",
      "If you're looking for general information about the field or have specific questions about a particular type of Healthcare software (such as Electronic Health Records (EHR) systems), feel free to ask, and I'll do my best to help! [Source: Leading the Charge The Future of Healthcare Software Solutions.pdf, Page 3]\n",
      "--------------------------------------------------\n",
      "\n",
      "User: what are th eproducts discussed in the pdf which are made by ion idea?\n",
      "\n",
      "AI Response:\n",
      "Unfortunately, the provided text does not mention specific products made by IonIde. It only mentions that they offer software solutions that drive the manufacturing industry for war. [Source: Navigating the Future Pioneering Trends in Manufacturing Software Development.pdf, Page 3]\n",
      "--------------------------------------------------\n",
      "\n",
      "User: hi\n",
      "\n",
      "AI Response:\n",
      "Hello! It seems like you're interested in learning more about IonIdea. However, I don't see your specific question asked in your message. Could you please clarify what you'd like to know about IonIdea? Are you looking for information on their healthcar e solutions, their products or services, or something else? [Source: Leading the Charge The Future of Healthcare Software Solutions.pdf, Page 3]\n",
      "--------------------------------------------------\n",
      "\n",
      "User:  what are th eproducts discussed in the pdf which are made by IonIdea?\n",
      "\n",
      "AI Response:\n",
      "The text does not specifically mention any products that are made by IonIdea. It only provides a general overview of the company's role and approach to innovation in the manufacturing industry, particularly for war-related products. Therefore, it is not possible to determine the specific products being discussed based on this information alone. [Source: Navigating the Future Pioneering Trends in Manufacturing Software Development.pdf, Page 3]\n",
      "--------------------------------------------------\n",
      "\n",
      "User: hi\n",
      "\n",
      "AI Response:\n",
      "Hello! It seems like you'd like to know more about IonIde or have some questions about their health care software. However, your message is quite brief and doesn't directly ask a specific question.\n",
      "\n",
      "If you could provide more context or clarify what you're looking for (e.g., information about their products, features, or services), I'd be happy to try and assist you further! [Source: Leading the Charge The Future of Healthcare Software Solutions.pdf, Page 3]\n",
      "--------------------------------------------------\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.core import Settings, SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "from llama_index.core.chat_engine import CondensePlusContextChatEngine\n",
    "from llama_index.core.extractors import SummaryExtractor, QuestionsAnsweredExtractor, TitleExtractor, KeywordExtractor\n",
    "import chromadb\n",
    "import os\n",
    "import re\n",
    "import uuid\n",
    "\n",
    "# 1. Initialize AI Model & Embeddings\n",
    "llm = Ollama(model=\"llama3.2\")\n",
    "embed_model = OllamaEmbedding(model_name=\"nomic-embed-text\")\n",
    "\n",
    "# 2. Set Global AI Settings\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "# 3. Initialize ChromaDB Client\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_db_IonIdea\")\n",
    "collection_name = \"document_knowledge_base\"\n",
    "collection = chroma_client.get_or_create_collection(collection_name)\n",
    "\n",
    "# 4. Initialize Chat Memory with Token Limit\n",
    "chat_memory = ChatMemoryBuffer(token_limit=2048)\n",
    "\n",
    "# 5. Load and Process Documents (Ensuring Proper Mapping)\n",
    "if collection.count() == 0:\n",
    "    print(\"No existing collection found. Extracting knowledge...\")\n",
    "    \n",
    "    documents = SimpleDirectoryReader(\n",
    "        input_dir=\"data\", \n",
    "        required_exts=[\".pdf\"],\n",
    "        filename_as_id=True,\n",
    "        recursive=True\n",
    "    ).load_data()\n",
    "    \n",
    "    print(f\"Loaded {len(documents)} documents\")\n",
    "    \n",
    "    # 6. Extract structured knowledge (Summaries + Q&A + Keywords)\n",
    "    node_parser = SentenceSplitter(chunk_size=1024, chunk_overlap=200, separator=\" \", paragraph_separator=\"\\n\\n\")\n",
    "    extractors = [\n",
    "        TitleExtractor(nodes=5),\n",
    "        QuestionsAnsweredExtractor(questions=3),\n",
    "        SummaryExtractor(summaries=[\"self\"]),\n",
    "        KeywordExtractor(),\n",
    "    ]\n",
    "    \n",
    "    index = VectorStoreIndex.from_documents(documents, node_parser=node_parser, extractors=extractors, show_progress=True)\n",
    "    \n",
    "    # Store parsed knowledge into ChromaDB\n",
    "    for doc in documents:\n",
    "        metadata = doc.metadata\n",
    "        filename = metadata.get(\"file_name\", \"Unknown File\")\n",
    "        page = metadata.get(\"page_label\", \"Unknown Page\")\n",
    "        content = doc.text\n",
    "        doc_id = str(uuid.uuid4())  # Generate unique ID\n",
    "        \n",
    "        collection.add(\n",
    "            ids=[doc_id],\n",
    "            documents=[content],\n",
    "            metadatas=[{\"file_name\": filename, \"page_label\": page}]\n",
    "        )\n",
    "        print(f\"Stored: {filename} - Page {page}\")\n",
    "else:\n",
    "    print(\"Existing collection found. Skipping extraction.\")\n",
    "\n",
    "# 7. System Prompt to Maintain Context-Only Responses\n",
    "system_prompt = \"\"\"\n",
    "You are an AI assistant designed to answer document-related queries. Your rules:\n",
    "1. Casual conversations (e.g., greetings) are allowed.\n",
    "2. You strictly refuse to answer any question that is outside the provided documents.\n",
    "3. When retrieving information, cite sources correctly with [Source: {file_name}, Page {page_label}].\n",
    "4. If the query has no relevant document, politely decline to answer.\n",
    "5. Generate responses based only on retrieved knowledge.\n",
    "\"\"\"\n",
    "\n",
    "# 8. Function for retrieving knowledge from ChromaDB\n",
    "def retrieve_knowledge(query):\n",
    "    results = collection.query(query_texts=[query], n_results=3)\n",
    "    \n",
    "    sources = []\n",
    "    for doc, meta_list in zip(results[\"documents\"], results[\"metadatas\"]):\n",
    "        if not meta_list:\n",
    "            continue\n",
    "        meta = meta_list[0]  # Extract first metadata dictionary\n",
    "        filename = meta.get(\"file_name\", \"Unknown File\")\n",
    "        page = meta.get(\"page_label\", \"Unknown Page\")\n",
    "        sources.append((filename, page, doc[0]))  # doc[0] as documents are lists\n",
    "    \n",
    "    return sources\n",
    "\n",
    "# 9. Function to clean retrieved text\n",
    "def clean_text(text):\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "# 10. Chat Function with Proper Citations\n",
    "def chat_with_proof(query):\n",
    "    print(f\"\\nUser: {query}\")\n",
    "    sources = retrieve_knowledge(query)\n",
    "    \n",
    "    if not sources:\n",
    "        print(\"AI: I'm sorry, but I can only answer questions related to the provided documents.\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nAI Response:\")\n",
    "    response_context = \"\"\n",
    "    for filename, page, excerpt in sources:\n",
    "        excerpt = clean_text(excerpt[:300])\n",
    "        response_context += f\"{excerpt}\\n\"\n",
    "    \n",
    "    if response_context:\n",
    "        chat_memory.put({\"query\": query, \"response\": response_context})\n",
    "        generated_response = llm.complete(response_context + \"\\n\\nBased on this, answer the user's query: \" + query)\n",
    "        print(f\"{generated_response.text} [Source: {filename}, Page {page}]\")\n",
    "    else:\n",
    "        print(\"AI: I cannot answer this question as it is outside the scope of the provided documents.\")\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# 11. Interactive Session\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Document Q&A System Initialized. Type 'exit' to quit.\")\n",
    "    while True:\n",
    "        user_query = input(\"\\nYour question: \")\n",
    "        if user_query.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        chat_with_proof(user_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing collection found. Skipping extraction.\n",
      "Document Q&A System Initialized. Type 'exit' to quit.\n",
      "\n",
      "User: what is ionidea?\n",
      "Generated Query Variations: [\"Since you didn't provide a specific query, I'll generate some general similar questions to improve document retrieval:\", '', '1. \"What is Ionide\\'s business model?\"']\n",
      "\n",
      "AI Response:\n",
      "IonIdea appears to be a software company that specializes in developing innovative solutions for the manufacturing industry, particularly in the context of Industry 4.0 (Internet of Things, or IoT). They offer software products and services that enable real-time communication between machines, tools, and components on factory floors, as well as addressing key challenges faced by the industry.\n",
      "\n",
      "Based on the provided text, it seems that IonIdea is a technology company focused on providing digital solutions for the manufacturing sector. [Source: Leading the Charge The Future of Healthcare Software Solutions.pdf, Page 3]\n",
      "--------------------------------------------------\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.core import Settings, SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "from llama_index.core.chat_engine import CondensePlusContextChatEngine\n",
    "from llama_index.core.extractors import SummaryExtractor, QuestionsAnsweredExtractor, TitleExtractor, KeywordExtractor\n",
    "import chromadb\n",
    "import os\n",
    "import re\n",
    "import uuid\n",
    "\n",
    "# 1. Initialize AI Model & Embeddings\n",
    "llm = Ollama(model=\"llama3.2\")\n",
    "embed_model = OllamaEmbedding(model_name=\"nomic-embed-text\")\n",
    "\n",
    "# 2. Set Global AI Settings\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "# 3. Initialize ChromaDB Client\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_db_IonIdea\")\n",
    "collection_name = \"document_knowledge_base\"\n",
    "collection = chroma_client.get_or_create_collection(collection_name)\n",
    "\n",
    "# 4. Initialize Chat Memory with Token Limit\n",
    "chat_memory = ChatMemoryBuffer(token_limit=2048)\n",
    "\n",
    "# 5. Load and Process Documents (Ensuring Proper Mapping)\n",
    "if collection.count() == 0:\n",
    "    print(\"No existing collection found. Extracting knowledge...\")\n",
    "    \n",
    "    documents = SimpleDirectoryReader(\n",
    "        input_dir=\"data\", \n",
    "        required_exts=[\".pdf\"],\n",
    "        filename_as_id=True,\n",
    "        recursive=True\n",
    "    ).load_data()\n",
    "    \n",
    "    print(f\"Loaded {len(documents)} documents\")\n",
    "    \n",
    "    # 6. Extract structured knowledge (Summaries + Q&A + Keywords)\n",
    "    node_parser = SentenceSplitter(chunk_size=1024, chunk_overlap=200, separator=\" \", paragraph_separator=\"\\n\\n\")\n",
    "    extractors = [\n",
    "        TitleExtractor(nodes=5),\n",
    "        QuestionsAnsweredExtractor(questions=3),\n",
    "        SummaryExtractor(summaries=[\"self\"]),\n",
    "        KeywordExtractor(),\n",
    "    ]\n",
    "    \n",
    "    index = VectorStoreIndex.from_documents(documents, node_parser=node_parser, extractors=extractors, show_progress=True)\n",
    "    \n",
    "    # Store parsed knowledge into ChromaDB\n",
    "    for doc in documents:\n",
    "        metadata = doc.metadata\n",
    "        filename = metadata.get(\"file_name\", \"Unknown File\")\n",
    "        page = metadata.get(\"page_label\", \"Unknown Page\")\n",
    "        content = doc.text\n",
    "        doc_id = str(uuid.uuid4())  # Generate unique ID\n",
    "        \n",
    "        collection.add(\n",
    "            ids=[doc_id],\n",
    "            documents=[content],\n",
    "            metadatas=[{\"file_name\": filename, \"page_label\": page}]\n",
    "        )\n",
    "        print(f\"Stored: {filename} - Page {page}\")\n",
    "else:\n",
    "    print(\"Existing collection found. Skipping extraction.\")\n",
    "\n",
    "# 7. System Prompt for Query Expansion\n",
    "query_expansion_prompt = \"\"\"\n",
    "You are an AI assistant that improves user queries for better document retrieval.\n",
    "Given a user query, generate 3 similar but varied questions to enhance understanding and correct errors.\n",
    "Examples:\n",
    "User: \"how does AI impact healthcare?\"\n",
    "Generated:\n",
    "1. \"What are the effects of AI on the healthcare industry?\"\n",
    "2. \"How is artificial intelligence used in modern medicine?\"\n",
    "3. \"What are the benefits and challenges of AI in healthcare?\"\n",
    "\"\"\"\n",
    "\n",
    "# 8. Function for generating query variations\n",
    "def generate_query_variations(query):\n",
    "    prompt = query_expansion_prompt + f\"\\nUser: \\\"{query}\\\"\\nGenerated:\"\n",
    "    response = llm.complete(prompt)\n",
    "    return response.text.strip().split(\"\\n\")[:3]  # Return top 3 variations\n",
    "\n",
    "# 9. Function for retrieving knowledge from ChromaDB\n",
    "def retrieve_knowledge(queries):\n",
    "    results = []\n",
    "    for query in queries:\n",
    "        query_results = collection.query(query_texts=[query], n_results=3)\n",
    "        for doc, meta_list in zip(query_results[\"documents\"], query_results[\"metadatas\"]):\n",
    "            if not meta_list:\n",
    "                continue\n",
    "            meta = meta_list[0]  # Extract first metadata dictionary\n",
    "            filename = meta.get(\"file_name\", \"Unknown File\")\n",
    "            page = meta.get(\"page_label\", \"Unknown Page\")\n",
    "            results.append((filename, page, doc[0]))  # doc[0] as documents are lists\n",
    "    return results\n",
    "\n",
    "# 10. Function to clean retrieved text\n",
    "def clean_text(text):\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "# 11. Chat Function with Query Expansion & Proper Citations\n",
    "def chat_with_proof(query):\n",
    "    print(f\"\\nUser: {query}\")\n",
    "    query_variations = generate_query_variations(query)\n",
    "    print(f\"Generated Query Variations: {query_variations}\")\n",
    "    \n",
    "    sources = retrieve_knowledge(query_variations)\n",
    "    \n",
    "    if not sources:\n",
    "        print(\"AI: I'm sorry, but I can only answer questions related to the provided documents.\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nAI Response:\")\n",
    "    response_context = \"\"\n",
    "    for filename, page, excerpt in sources:\n",
    "        excerpt = clean_text(excerpt[:300])\n",
    "        response_context += f\"{excerpt}\\n\"\n",
    "    \n",
    "    if response_context:\n",
    "        chat_memory.put({\"query\": query, \"response\": response_context})\n",
    "        generated_response = llm.complete(response_context + \"\\n\\nBased on this, answer the user's query: \" + query)\n",
    "        print(f\"{generated_response.text} [Source: {filename}, Page {page}]\")\n",
    "    else:\n",
    "        print(\"AI: I cannot answer this question as it is outside the scope of the provided documents.\")\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# 12. Interactive Session\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Document Q&A System Initialized. Type 'exit' to quit.\")\n",
    "    while True:\n",
    "        user_query = input(\"\\nYour question: \")\n",
    "        if user_query.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        chat_with_proof(user_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
