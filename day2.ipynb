{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "llm = Ollama(model=\"llama3.2\", request_timeout=120.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(metadata={'prompt_type': <PromptType.CUSTOM: 'custom'>}, template_vars=['context_str', 'querry_str'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, template='This is a context information below:\\n            ---------------------------------------\\n            \"{context_str}\"\\n            ----------------------------------------\\n            Based on the context you need to answer without making any kind of assumption,\\n            if you think that the info is not enough to answer the users question you have to say that you don\\'t know answer for that.\\n            please Answer the Question : {querry_str}\\n            ')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "template = (\"\"\"This is a context information below:\n",
    "            ---------------------------------------\n",
    "            \"{context_str}\"\n",
    "            ----------------------------------------\n",
    "            Based on the context you need to answer without making any kind of assumption,\n",
    "            if you think that the info is not enough to answer the users question you have to say that you don't know answer for that.\n",
    "            please Answer the Question : {querry_str}\n",
    "            \"\"\")\n",
    "\n",
    "qa_template = PromptTemplate(template)\n",
    "qa_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what is this about?\"\n",
    "prompt= qa_template.format(context_str= \"\",querry_str = question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.complete(prompt, max_tokens=100, stop_sequence=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know. The provided text doesn't contain any specific information or context that would allow me to determine the topic of discussion.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
