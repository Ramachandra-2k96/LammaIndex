{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract MetaData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.core import Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import MetadataMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.llm = Ollama(model = \"granite3.2:2b\",request_timeout=600)\n",
    "Settings.embed_model = OllamaEmbedding(model_name=\"nomic-embed-text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.extractors import (SummaryExtractor,QuestionsAnsweredExtractor,TitleExtractor,KeywordExtractor,BaseExtractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install llama-index-extractors-entity --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.extractors.entity import EntityExtractor\n",
    "from llama_index.core.node_parser import TokenTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_splitter = TokenTextSplitter(separator=' ', chunk_size=1200, chunk_overlap=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractors = [\n",
    "    TitleExtractor(nodes = 5),\n",
    "    QuestionsAnsweredExtractor(questions=3),\n",
    "    SummaryExtractor(summaries=[\"self\"]),\n",
    "    KeywordExtractor(),\n",
    "    # EntityExtractor(),\n",
    "    # BaseExtractor()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformations = [test_splitter] + extractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "docs = SimpleDirectoryReader(input_files=['./data/Tulu_Language_Text_Recognition_and_Translation.pdf']).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.ingestion import IngestionPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_pipeline(documents):\n",
    "    pipeline = IngestionPipeline(transformations=transformations)\n",
    "    nodes = await pipeline.arun(documents=documents)\n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.28s/it]\n",
      "100%|██████████| 2/2 [00:03<00:00,  1.99s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.51s/it]\n",
      "100%|██████████| 1/1 [00:04<00:00,  4.47s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.23s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.01it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.26it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.32it/s]\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.55s/it]\n",
      "100%|██████████| 2/2 [00:06<00:00,  3.27s/it]\n",
      "100%|██████████| 2/2 [00:04<00:00,  2.18s/it]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.71s/it]\n",
      "100%|██████████| 15/15 [01:27<00:00,  5.82s/it]\n",
      "100%|██████████| 15/15 [02:09<00:00,  8.62s/it]\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "nodes = asyncio.run(run_pipeline(documents=docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, Settings\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.node_parser import SimpleNodeParser\n",
    "from llama_index.core.schema import TextNode\n",
    "from llama_index.llms.ollama import Ollama\n",
    "import asyncio\n",
    "\n",
    "# System prompt for LLM to decide response type\n",
    "system_prompt = \"\"\"\n",
    "You are an AI assistant capable of both general conversation and retrieving information from a specific PDF document.\n",
    "If a query requires information from the PDF, retrieve and return relevant details from the indexed document.\n",
    "Otherwise, respond as a general AI assistant.\n",
    "\"\"\"\n",
    "\n",
    "# Create an index from extracted nodes\n",
    "def create_index_from_nodes(nodes):\n",
    "    index = VectorStoreIndex(nodes)\n",
    "    return index\n",
    "\n",
    "# Function to generate a response where the LLM decides the response type\n",
    "async def generate_response(query, index=None):\n",
    "    prompt = f\"{system_prompt}\\nUser Query: {query}\\n\"\n",
    "    if index:\n",
    "        retriever = index.as_retriever()\n",
    "        query_engine = RetrieverQueryEngine(retriever=retriever)\n",
    "        retrieved_info = await query_engine.aquery(query)\n",
    "        prompt += f\"\\nRetrieved Information:\\n{retrieved_info.response}\"\n",
    "    return await Settings.llm.acomplete(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.24s/it]\n",
      "100%|██████████| 2/2 [00:06<00:00,  3.41s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.39s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.44it/s]\n",
      "100%|██████████| 1/1 [00:07<00:00,  7.14s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.03s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.53it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.55it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.31it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.11it/s]\n",
      "100%|██████████| 2/2 [00:03<00:00,  1.67s/it]\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.44s/it]\n",
      "100%|██████████| 15/15 [01:36<00:00,  6.42s/it]\n",
      "100%|██████████| 15/15 [02:12<00:00,  8.85s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Usage example (Assuming nodes are preprocessed)\n",
    "nodes = asyncio.run(run_pipeline(documents=docs))  # Extracted metadata nodes\n",
    "index = create_index_from_nodes(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the information from Volume 2023's study \"Comparative Analysis and Performance Evaluation of Deep Learning Models vs. Rule-Based Methods for Tulu Language's 'a' Character Recognition and Correlated Blue Score Assessment,\" here's how CNN compares to rule-based translation:\n",
      "\n",
      "1. **Character Recognition Accuracy**: The Convolutional Neural Network (CNN) model surpasses traditional rule-based translations with an accuracy of 92% on the validation set, significantly higher than other algorithms. This indicates that deep learning models like CNN are more effective at recognizing 'a' characters in this context.\n",
      "\n",
      "2. **Performance Metrics**: The CNN outperforms rule-based methods in terms of f1-score (harmonic mean of precision and recall). F1-scores around 90% demonstrate that the CNN offers superior, consistent accuracy across different character classifications compared to rule-based translations.\n",
      "\n",
      "3. **Impact on 'Blue Score'**: The Blue score, a measure of quality or reliability for this recognition task, influences both deep learning models and conventional rule-based methods (refer to FIGURE 16 in the study). CNN performs well within these constraints, emphasizing its ability to maintain high standards despite varying conditions.\n",
      "\n",
      "Thus, when considering Tulu language character recognition and translation for 'a' characters, especially taking into account performance metrics like accuracy and f1-score along with influence on Blue score, deep learning models like the CNN demonstrate superiority over rule-based translations.\n"
     ]
    }
   ],
   "source": [
    "# Sample user query\n",
    "user_query = \"How does CNN compare to rule-based translation in the PDF?\"\n",
    "response = asyncio.run(generate_response(user_query, index))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm an AI and don't have feelings, but I'm here to engage in conversation and provide information as needed. Let me tell you a story now:\n",
      "\n",
      "In the verdant hills of Karnataka, where ancient Dravidian scripts thrived, there was a team of dedicated scholars who took on the mission to preserve an important language - Tulu. Their hearts were filled with reverence for this native tongue, threatened by dominant languages like Kannada.\n",
      "\n",
      "These scholars crafted a state-of-the-art machine learning model, a deep convolutional neural network (CNN), as their eyes into the world of Tulu script. This wasn't an ordinary algorithm; it was meticulously trained with 30,500 handwritten Tulu characters. This digital sentinel could perceive and decipher Tulu text remarkably like humans.\n",
      "\n",
      "One crisp morning, they tested this creation to see how well it could translate complexities of the Tulu script into a language that could be understood globally - English or vice versa. They were delighted as their model not only translated words but also captured the subtle nuances and context of Tulu, maintaining these elements in its digital understanding.\n",
      "\n",
      "Beyond this robust CNN, they employed other machine learning tools: Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks. These were vital for capturing the intricate word structures and sentence patterns of Tulu, ensuring every detail was translated with precision.\n",
      "\n",
      "Their approach wasn't just about technology; it celebrated linguistic heritage they sought to safeguard. It harmoniously blended traditional rule-based systems with deep learning's adaptability, creating a unified framework that honored both the age of ancient scripts and modern innovation.\n",
      "\n",
      "In their digital domain of algorithms and language maps, these scholars ensured Tulu storytelling continued, one character at a time.\n"
     ]
    }
   ],
   "source": [
    "user_query = \"How are you? can you tell me a story please...\"\n",
    "response = asyncio.run(generate_response(user_query, index))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am Granite, an AI assistant developed by IBM. I specialize in understanding and explaining complex topics based on the data provided to me. In this context, we're discussing machine learning models designed for deciphering unique scripts like Tulu, with a focus on improving translation outcomes and detecting emotions from historical documents.\n"
     ]
    }
   ],
   "source": [
    "user_query = \"Who are you?\"\n",
    "response = asyncio.run(generate_response(user_query, index))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The authors of this PDF are:\n",
      "\n",
      "1. Manimozhi\n",
      "2. Seshikala et al.\n",
      "3. Anush Bijoor\n",
      "4. Savitha (or a team)\n",
      "5. Rao et al.\n",
      "6. Memon et al.\n",
      "7. Albahli\n",
      "8. Bora et al.\n",
      "9. Deore et al.\n",
      "10. Khandokar et al.\n",
      "11. Guha et al.\n",
      "12. Hamdan et al. (possibly repeated)\n",
      "13. Vinjit et al.\n",
      "14. Athira\n",
      "15. Yadav et al.\n",
      "\n",
      "(Note: There is some repetition in the list of authors, possibly due to multiple authors contributing to various sections or aspects within this document.)\n"
     ]
    }
   ],
   "source": [
    "user_query = \"who are the authors of the pdf attched?\"\n",
    "response = asyncio.run(generate_response(user_query, index))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.59s/it]\n",
      "\n",
      "\u001b[A\n",
      "100%|██████████| 2/2 [00:14<00:00,  7.02s/it]\n",
      "100%|██████████| 1/1 [00:14<00:00, 14.91s/it]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:16<00:00,  8.42s/it]\n",
      "100%|██████████| 1/1 [00:14<00:00, 14.14s/it]\n",
      "\n",
      "100%|██████████| 1/1 [00:12<00:00, 12.47s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.77s/it]\n",
      "100%|██████████| 3/3 [00:31<00:00, 10.56s/it]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:31<00:00, 31.94s/it]\n",
      "100%|██████████| 3/3 [00:44<00:00, 14.93s/it]\n",
      "100%|██████████| 1/1 [00:40<00:00, 40.93s/it]\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.21s/it]\n",
      "100%|██████████| 1/1 [00:08<00:00,  8.84s/it]\n",
      "100%|██████████| 2/2 [00:10<00:00,  5.32s/it]\n",
      "100%|██████████| 6/6 [01:12<00:00, 12.04s/it]\n",
      "100%|██████████| 6/6 [01:18<00:00, 13.06s/it]\n",
      "100%|██████████| 6/6 [01:40<00:00, 16.67s/it]\n",
      "100%|██████████| 6/6 [01:44<00:00, 17.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response 1: Based on the information from the PDF:\n",
      "\n",
      "**Comparison of CNN and Rule-Based Translation:**\n",
      "\n",
      "1. **Architecture**:\n",
      "   - CNN layers have varying configurations, including filter numbers, kernel sizes, dropout rates, max pooling operations, and dense layers with specific activation functions (ReLU or Softmax). These characteristics enable more nuanced understanding of language structures compared to rule-based systems, which typically rely on predefined rules for translation.\n",
      "\n",
      "2. **Hyperparameters**:\n",
      "   - For the rule-based system: Batch size = 32, Loss function = Categorical Cross-Entropy, Learning rate = 0.001, Epochs = 150, and Optimizer = Adam. These hyperparameters were likely chosen to optimize the performance of this neural network component within a language translation context but are not as flexible for handling diverse linguistic nuances as CNN does through its deep learning architecture.\n",
      "\n",
      "3. **Rule-Based Translation System**:\n",
      "   - This system functions in four sequential modules: input preparation, Named Entity Recognition using Spacy (for POS tagging), translation from English to Kannada and back, and finally Tulu sentence construction based on Kannada structure while preserving previously tagged named entities. The human intervention for manual handling of English language components like named entities is a notable distinction.\n",
      "\n",
      "4. **Strengths of CNN**:\n",
      "   - Automatic feature extraction from raw text data allows CNN to capture intricate linguistic patterns and nuances effectively, often outperforming rule-based systems that depend on explicit programming of each step due to their adaptability and ability to learn complex word and phrase relationships within sentences.\n",
      "\n",
      "5. **Limitations of Rule-Based System**:\n",
      "   - The reliance on manually designed grammar for translation between languages can become cumbersome as linguistic complexity increases or if new language patterns emerge. This highlights the distinct advantage CNN holds, which is its inherent adaptability and capacity to learn from data—a significant benefit in dynamic linguistic contexts.\n",
      "\n",
      "In conclusion, while rule-based translation systems excel at managing known language translations through predefined rules, deep learning models like CNN generally exhibit superior performance due to their automated feature extraction and capacity for handling complex linguistic patterns. This comparison suggests that Neural Machine Translation (NMT) with its neural network approach may offer further advantages over traditional rule-based methods by potentially learning more abstract relationships between languages in a data-driven manner.\n",
      "Response 2: I'm an artificial intelligence, so I don't have feelings or emotions; I can only provide information and respond as best I can. Now, let me tell you a story inspired by your query:\n",
      "\n",
      "In the heart of Western Ghats, where ancient Tulu script was revered in manuscripts, lived Dr. Jyothi. She was an expert in computer vision and machine learning, leading a dedicated team to preserve this rich cultural legacy digitally.\n",
      "\n",
      "Their mission: To translate centuries-old Tulu manuscripts into digital formats using handwritten character recognition. This wasn't just about creating copies; it was about ensuring these age-old stories wouldn't fade away with time. The challenge? Tulu's small script demanded meticulous preparation and cutting-edge tools to capture its unique complexities.\n",
      "\n",
      "Dr. Jyothi and her team started by compiling an extensive dataset of 30,500 carefully transcribed Tulu characters—each one a testament to the script's richness and peculiarities. \n",
      "\n",
      "They then experimented with various algorithms for recognition. Traditional rule-based systems, programmed with deep linguistic knowledge about Tulu, worked alongside neural machine translation (NMT), renowned for efficient text translation between languages. NMT proved particularly effective due to the complexities of Tulu's language structure.\n",
      "\n",
      "Among their tools was a Convolutional Neural Network (CNN), known for its ability to derive hierarchical features directly from images, making it exceptionally effective in deciphering these ancient handwriting components. To strengthen Tulu-to-English translations, they developed a hybrid approach integrating rule-based systems and NMT, leveraging each for handling specific linguistic nuances.\n",
      "\n",
      "Beyond recognition, the team crafted an Encoder-Decoder model combined with Long Short-Term Memory (LSTM). This powerful system enabled them to reverse translate—convert Tulu sentences back into English. It was a remarkable feat considering the distinct nature of this minority script.\n",
      "\n",
      "Dr. Jyothi and her crew merged traditional wisdom with technological prowess, ensuring that precious Tulu knowledge wouldn’t be lost to time. Through their work, they not only advanced computational methods for recognizing handwritten scripts but also built a bridge between cultural heritage and contemporary technology—showcasing how AI can serve as an enduring guardian of human history.\n",
      "Response 3: I am an AI assistant designed for both general conversation and retrieval of specific information from a PDF document. When your query requires information indexed in the PDF, I can provide relevant details. However, as a general AI, my primary role is to understand and generate human-like responses based on the input data provided or prompted by you.\n",
      "Response 4: The authors of the PDF attached likely comprise an interdisciplinary group with expertise in Natural Language Processing (NLP), Computational Linguistics, Computer Vision, Historical Linguistics, and specialists working on ancient scripts such as Devanagari or Dravidian. To find the precise list of authors, please open the PDF and check for acknowledgments at the document's beginning, where this information is usually listed.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.core import Settings\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Initialize LlamaIndex settings\n",
    "Settings.llm = Ollama(model=\"granite3.2:2b\", request_timeout=600)\n",
    "Settings.embed_model = OllamaEmbedding(model_name=\"nomic-embed-text\")\n",
    "\n",
    "# Define extractors and text splitter\n",
    "from llama_index.core.extractors import (\n",
    "    SummaryExtractor,\n",
    "    QuestionsAnsweredExtractor,\n",
    "    TitleExtractor,\n",
    ")\n",
    "# Uncomment if needed: from llama_index.extractors.entity import EntityExtractor\n",
    "from llama_index.core.node_parser import TokenTextSplitter\n",
    "\n",
    "test_splitter = TokenTextSplitter(separator=' ', chunk_size=1200, chunk_overlap=100)\n",
    "extractors = [\n",
    "    TitleExtractor(nodes=5),\n",
    "    QuestionsAnsweredExtractor(questions=4),\n",
    "    SummaryExtractor(summaries=[\"self\"]),\n",
    "    # KeywordExtractor(),\n",
    "    # EntityExtractor(),\n",
    "    # BaseExtractor()\n",
    "]\n",
    "transformations = [test_splitter] + extractors\n",
    "\n",
    "# Read documents from PDF\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "docs = SimpleDirectoryReader(input_files=['./data/Tulu_Language_Text_Recognition_and_Translation.pdf']).load_data()\n",
    "\n",
    "# Batch processing: create nodes and build the vector index in one step\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core import VectorStoreIndex\n",
    "import asyncio\n",
    "\n",
    "async def create_index(documents, batch_size=5):\n",
    "    pipeline = IngestionPipeline(transformations=transformations)\n",
    "    # Divide documents into batches\n",
    "    batches = [documents[i:i+batch_size] for i in range(0, len(documents), batch_size)]\n",
    "    # Process batches concurrently\n",
    "    tasks = [pipeline.arun(documents=batch) for batch in batches]\n",
    "    batch_nodes = await asyncio.gather(*tasks)\n",
    "    # Flatten the list of lists into a single list of nodes\n",
    "    all_nodes = [node for batch in batch_nodes for node in batch]\n",
    "    index = VectorStoreIndex(all_nodes)\n",
    "    return index\n",
    "\n",
    "# System prompt for LLM to decide the response type\n",
    "system_prompt = \"\"\"\n",
    "You are an AI assistant capable of both general conversation and retrieving information from a specific PDF document.\n",
    "If a query requires information from the PDF, retrieve and return relevant details from the indexed document.\n",
    "Otherwise, respond as a general AI assistant.\n",
    "\"\"\"\n",
    "\n",
    "# Function to generate a response using the system prompt and, if available, retrieved PDF info\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.schema import TextNode\n",
    "\n",
    "async def generate_response(query, index=None):\n",
    "    prompt = f\"{system_prompt}\\nUser Query: {query}\\n\"\n",
    "    if index:\n",
    "        retriever = index.as_retriever()\n",
    "        query_engine = RetrieverQueryEngine(retriever=retriever)\n",
    "        retrieved_info = await query_engine.aquery(query)\n",
    "        prompt += f\"\\nRetrieved Information:\\n{retrieved_info.response}\"\n",
    "    return await Settings.llm.acomplete(prompt)\n",
    "\n",
    "# Usage example: create the index and generate responses\n",
    "index = asyncio.run(create_index(docs, batch_size=5))\n",
    "\n",
    "# Example queries:\n",
    "user_query = \"How does CNN compare to rule-based translation in the PDF?\"\n",
    "response = asyncio.run(generate_response(user_query, index))\n",
    "print(\"Response 1:\", response)\n",
    "\n",
    "user_query = \"How are you? Can you tell me a story please...\"\n",
    "response = asyncio.run(generate_response(user_query, index))\n",
    "print(\"Response 2:\", response)\n",
    "\n",
    "user_query = \"Who are you?\"\n",
    "response = asyncio.run(generate_response(user_query, index))\n",
    "print(\"Response 3:\", response)\n",
    "\n",
    "user_query = \"Who are the authors of the pdf attached?\"\n",
    "response = asyncio.run(generate_response(user_query, index))\n",
    "print(\"Response 4:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
