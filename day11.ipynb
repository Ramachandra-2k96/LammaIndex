{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract MetaData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.core import Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import MetadataMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.llm = Ollama(model = \"granite3.2:2b\",request_timeout=600)\n",
    "Settings.embed_model = OllamaEmbedding(model_name=\"nomic-embed-text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.extractors import (SummaryExtractor,QuestionsAnsweredExtractor,TitleExtractor,KeywordExtractor,BaseExtractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install llama-index-extractors-entity --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.extractors.entity import EntityExtractor\n",
    "from llama_index.core.node_parser import TokenTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_splitter = TokenTextSplitter(separator=' ', chunk_size=1200, chunk_overlap=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractors = [\n",
    "    TitleExtractor(nodes = 5),\n",
    "    QuestionsAnsweredExtractor(questions=3),\n",
    "    SummaryExtractor(summaries=[\"self\"]),\n",
    "    KeywordExtractor(),\n",
    "    # EntityExtractor(),\n",
    "    # BaseExtractor()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformations = [test_splitter] + extractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "docs = SimpleDirectoryReader(input_files=['./data/Tulu_Language_Text_Recognition_and_Translation.pdf']).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='655d59ca-3a76-4b11-9f03-ca7aa9eb30a3', embedding=None, metadata={'page_label': '1', 'file_name': 'Tulu_Language_Text_Recognition_and_Translation.pdf', 'file_path': 'data/Tulu_Language_Text_Recognition_and_Translation.pdf', 'file_type': 'application/pdf', 'file_size': 1427571, 'creation_date': '2025-02-28', 'last_modified_date': '2025-02-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"Tulu Language Text Recognition and\\nTranslation\\nPRATHWINI1, ANISHA P RODRIGUES2, P. VIJAYA3, ROSHAN FERNANDES4*\\n1Department of Master of Computer Applications, NMAM Institute of Technology, NITTE(Deemed to be University), India\\n(e-mail:prathwini.devadiga@nitte.edu.in)\\n2Department of Computer Science and Engineering, NMAM Institute of Technology, NITTE(Deemed to be University), India (e-mail:anishapr@nitte.edu.in)\\n3Department of Mathematics and Computer Science, Modern College of Business and Sciences, Bowshar, Sultanate of Oman (e-mail:pvvijaya@gmail.com)\\n4Department of Computer Science and Engineering, NMAM Institute of Technology, NITTE(Deemed to be University), India\\n(e-mail:roshan_nmamit@nitte.edu.in)\\nCorresponding author: Roshan Fernandes (e-mail: roshan_nmamit@nitte.edu.in)\\nABSTRACT Language is a primary means of communication, but it is not the only means; knowing a\\nlanguage does, however, assist speed up the process. Many distinct languages are spoken worldwide, and\\npeople use them to communicate. This is only one of the many reasons why language is so crucial. Based\\non the literature survey, it is evident that there is a lack of available translators for the Tulu language.\\nDespite being prevalent predominantly in Karnataka, the Tulu language has not been as widely spoken as\\nother Indian languages until recently, although it gained enough recognition to become the second language\\nin Karnataka. The purpose of our research work aims at translating the English language into the Tulu\\nlanguage. During the evaluation the system was tested on a dataset consisting of handwritten characters\\nduring the evaluation process Convolutional Neural Networks used achieved an accuracy rate of 92%. To\\ntranslate English to the Tulu language, we employed a parallel sentence dataset for the neural approach and\\na parallel word dataset for the rule-based approach. The rule-based approach resulted in an 89% accuracy\\nrate for word-based analysis and an 81% accuracy rate for sentence-based analysis for the English-to-Tulu\\nlanguage translation. The neural machine translation approach of the Encoder-Decoder model with LSTM\\nis been used to accomplish translation from English to Tulu with a BLEU score of 0.83 and Tulu to English\\nwith a BLUE score of 0.65. The model also employed hybrid machine translation to enhance the translation.\\nINDEX TERMS Machine Translation, Rule-based method, Neural Network Translation, Convolutional\\nNeural Network (CNN), Encoder-Decoder Model, and Long short-term memory (LSTM)\\nI. INTRODUCTION\\nLanguage, as a means of communication, encompasses a\\ncollection of written symbols and sounds used by individ-\\nuals within a specific region or country for oral or written\\nexpression. It sets humans apart and involves acquiring a\\ncomplex framework of vocabulary, structure, and syntax for\\neffective communication. Indian languages are categorized\\ninto families such as Indo-Iranian or Indo-European, Munda,\\nDravidian, Austroasiatic, Sino-Tibetan, and Tibeto-Burman.\\nThe Indian constitution mentions twenty-two of these lan-\\nguages. India’s multilingual nature is evident across its states,\\nalthough fluency in every language spoken within the nation\\nisn’t universal. Tulu, from the Dravidian family, is spoken\\npredominantly in the southern region of Dakshina Kannada\\nand Udupi districts in Karnataka, and in Kasaragod district\\nin southwestern India. Tuluva, the indigenous people, reside\\nin Tulu Nadu. Presently, Tulu is formally acknowledged as\\nthe second language of Karnataka, with ongoing efforts to\\ninclude it in the 8th Schedule of the Constitution. It contains\\nfour dialects, mainly used for inter-community communica-\\ntion, trade, and entertainment. Tulu is spoken across various\\nregions like Mangalore, Udupi, Karkala, Belthangady, Kun-\\ndapura, Kasaragod, Manjeshwar, Puttur Sullia, and Bantwal,\\nwith different dialects in each area. Efforts are underway to\\ninclude Tulu in the Constitution’s 8th Schedule. Machine\\ntranslation (MT), a crucial aspect of natural language pro-\\ncessing, has evolved significantly. Rule-based machine trans-\\nlation (RBMT) is an early approach to language translation\\nusing predefined linguistic rules. These systems analyze in-\\nput sentences, break them down into grammatical parts, and\\nthen apply linguistic rules to generate the translated output in\\na target language. RBMT operates in three phases: analyzing\\nthe input sentence, transferring the linguistic elements into\\nthe target language, and finally, generating the translated sen-\\nVOLUME , 2023 1\\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3355470\\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='7721022e-4b69-44b9-afbe-12de29d392e4', embedding=None, metadata={'page_label': '2', 'file_name': 'Tulu_Language_Text_Recognition_and_Translation.pdf', 'file_path': 'data/Tulu_Language_Text_Recognition_and_Translation.pdf', 'file_type': 'application/pdf', 'file_size': 1427571, 'creation_date': '2025-02-28', 'last_modified_date': '2025-02-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"tence. However, RBMT’s reliance on manually constructed\\nrules limited its adaptability to handle the complexities of\\nnatural languages. This led to the development of more\\nadvanced translation methods like neural machine translation\\n(NMT). End-to-end neural machine translation (NMT) has\\nbecome dominant, departing from earlier rule-based systems\\nthat heavily relied on manually crafted translation rules and\\nlinguistic knowledge. NMT utilizes computer systems to\\ntranslate languages and has shown remarkable success com-\\npared to earlier rule-based methods, which required exten-\\nsive linguistic rules for translation between languages. NMT\\nhas streamlined translation processes and improved results.\\nFollowing are some of the significant contributions of the\\nproposed work:\\n• The data set, consisting of 30,500 manually collected\\nTulu handwritten characters, was compiled for the pro-\\nposed work.\\n• To recognize the Tulu language script, both the CNN\\nalgorithm and other machine learning algorithms were\\nutilized.\\n• A dataset comprising 1458 words and 1000 English-to-\\nTulu sentences was manually collected. Rule-based and\\nneural machine techniques were employed to translate\\nEnglish to Tulu.\\n• The English-to-Tulu language translation was accom-\\nplished through the application of both rule-based and\\nneural machine techniques.\\n• To achieve backward translation from Tulu to English,\\nthe Encoder-Decoder model with LSTM has been uti-\\nlized, employing the neural machine translation ap-\\nproach.\\n• Hybrid machine translation was employed by combin-\\ning rule-based and neural machine translation to im-\\nprove the performance of the model.\\nII. RELATED WORKS\\nIn this section, we elaborate on our efforts in handwritten\\ncharacter recognition and English-to-Tulu language trans-\\nlation, employing various methods. To address the identi-\\nfication of human emotions, we conduct a literature sur-\\nvey to understand and pinpoint the limitations of current\\ntechniques. [1] Manimozhi’s software focuses on precise\\nrecognition of handwritten Tulu characters, aiding in trans-\\nforming historical manuscripts. [2] Seshikala et al.’s study\\nevaluates CNN models for Devanagari characters, emphasiz-\\ning computational efficiency in their specialized architecture.\\n[3] Anush Bijoor’s research rejuvenates the ancient Tulu\\nalphabet using CNNs for character recognition, aiming to\\ntransform historical manuscripts. [4] Savitha et al.’s method\\nutilizes image preprocessing techniques to enhance character\\nrecognition accuracy. [5] Rao et al. explore machine-learning\\nmethods for Tulu characters, emphasizing CNN’s efficiency\\nin recognition. [6] Memon et al. present a user-friendly\\napproach for Kannada character recognition employing deep\\nlearning algorithms. [7] Albahli’s summary explores hand-\\nwritten document recognition and potential future research\\ndirections based on an SLR. [8] Bora et al. propose a multi-\\nstep approach using enhanced Faster-RCNN for numeral\\nrecognition from images. [9] Deore et al. combine CNN and\\nECOC classifiers for accurate OCR of handwritten charac-\\nters. [10] Khandokar et al. introduce a dataset and a VGG16\\nmodel for Devanagari character recognition. [11] Guha et al.\\nexplore CNN’s capacity in recognizing complex handwritten\\ncharacters with a dataset achieving 92.91% accuracy. [12]\\nRani et al. emphasize CNN’s role in automatic feature ex-\\ntraction for character recognition, specifically in Devanagari\\nscript. [13] Hamdan et al. design a capsule network for\\neffective Kannada character recognition. [14] Vinjit et al.\\ncompare various approaches for handwriting recognition,\\nincluding statistical methods and neural networks. [15] Rani\\net al. highlight the need for improved accuracy and effi-\\nciency in Handwritten Character Recognition.[16] Athira et\\nal. use transfer learning from Devanagari for recognizing\\nhandwritten Kannada characters. [17] Yadav et al. tackle\\nthe challenge of recognizing confusing characters in Kan-\\nnada documents using templates and classifiers. [18] Ganji\\net al. comprehensively cover phases of offline handwritten\\nHindi character recognition. [19] Ayyob et al. discuss OCR\\nchallenges in recognizing Telugu literature characters due to\\nlimited datasets and trained CNNs. [20] Srelekha et al. survey\\ndiverse methods used in Malayalam handwriting recogni-\\ntion, highlighting feature extraction and classification tech-\\nniques. [21] Md. Adnanul Islam et al. propose an effective\\nBengali-to-English translation technique, improving machine\\ntranslation accuracy. [22] Neha Bhadwal et al. propose a\\nsystem for translating Hindi text into Sanskrit, considering\\nlinguistic features of both languages. [23] Sitender et al.\\nutilize bilingual dictionaries and rule-based approaches for\\ntranslation. [24] Namrata Kharate et al. discuss challenges\\nin building translation models due to language differences\\nin syntax and morphology. [25] Kodabagi et al. focus on\\nneural machine translation (NMT) for Kannada to English,\\nachieving higher accuracy than statistical methods. [26]\\nSalunkhe et al. highlight challenges in translating complex\\nsentences and suggest simplification techniques.[27] Arikpo\\net al. propose a method for localizing and classifying digits\\ninto ten classes using Faster-RCNN. [28] Mardhotillah et al.\\npropose a methodology achieving a high accuracy of 97.56%\\nin translating sentences into Kannada.[29] Dhar et al. propose\\na system using parallel datasets for document translation.\\n[30] Soman et al. develop a transfer-based translator for\\nEnglish to Efik language translation. [31] Prajapati’s paper\\nevaluates CNN models for recognizing Devanagari charac-\\nters, highlighting the absence of a universal model. [32]\\nChoudhary et al. compare subword segmentation methods for\\ntranslating English to Dravidian languages. [33] Gogineni et\\nal. assess an NMT architecture’s effectiveness for translation,\\nemphasizing the impact of lengthy English sentences. [34]\\nAsha Hegde et al. explore pragmatic methods for examining\\nmachine translation between Tulu and Kannada. [35][36]\\nRoshan Fernandes et al. propose models recognizes hand-\\nwritten Kannada characters and predict emotions in videos.\\n2 VOLUME , 2023\\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3355470\\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='67a0fb2b-1dc3-4898-8d0d-e6d897b3436b', embedding=None, metadata={'page_label': '3', 'file_name': 'Tulu_Language_Text_Recognition_and_Translation.pdf', 'file_path': 'data/Tulu_Language_Text_Recognition_and_Translation.pdf', 'file_type': 'application/pdf', 'file_size': 1427571, 'creation_date': '2025-02-28', 'last_modified_date': '2025-02-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='[37] Zixin Dou et al. explored various machine learning\\nalgorithms and achieved favorable accuracy results. From the\\nliterature survey we have observed that Hybrid algorithms\\nare not considered for the evaluation of the handwritten Tulu\\ncharacter recognition. Expansion of the dataset has to be done\\nto increase the accuracy of the Tulu character recognition\\nmodel. More Preprocessing techniques can be applied to in-\\ncrease the accuracy of the Tulu character recognition model.\\nFrom the literature survey we have also found that there are\\nno existing translators for English to Tulu or Tulu to English\\nsince Tulu is a local language which is spoken in udupi\\nand Mangaluru region. Our research objective is to create\\na system that can translate the English language into Tulu,\\nas there are no existing translators available according to the\\nliterature survey. Our research endeavors to create a system\\nwith the goal to fill the gap in Tulu language translation\\ntools by translating English to Tulu and Tulu to English. To\\naddress the lack of Tulu language translators by developing a\\nsystem that can precisely translate English to Tulu and Tulu\\nto English, as no existing translators were identified during\\nthe literature survey.\\nIII. METHODOLOGY\\nA. TULU CHARACTER RECOGNITION\\nThe proposed method, as represented in Figure 1, follows\\nvarious steps including Image Pre-processing, Image Seg-\\nmentation, Feature Extraction, Classification using machine\\nlearning algorithms, and Performance Comparison.\\n1) Data Preparation and Collection\\nFigure 1 describes the methodology used for Tulu character\\nrecognition such as image collection which involves collect-\\ning various handwritten characters of about 62 classes which\\nincludes 50 characters and 12 numerical. Total 31000 images\\nwhich include 500 images of each class are collected.\\n2) Image Preprocessing\\nAugmentation techniques are being applied with rota-\\ntion_range = 40, zoom_range = 0.1, and brightness range\\nbetween 0.5, and 0.8 which are shown in figure 2. Below\\nare the different augmented images. Once the character is\\ndetected each character is cropped individually and converted\\ninto 28*28 pixels. Each image is converted into a Grayscale\\nand then a binary image to reduce the noise in the image\\nwhich is shown in figure 3.\\n(a) Ordinary Image (b) Grey Image (c) Binary Image\\nFIGURE 3. Image Color Conversion\\nAlgorithm 1Image Segmentation\\nSet image width to 1000\\nSet image height to 550\\nSet dim to (width, height)\\nResize the image to dim using cv2.INTER_AREA\\nConvert the resized image to grayscale\\nSet thresh to 90\\nCreate a binary image using thresholding with the OTSU\\nmethod on binaryImage\\nSet kernel to a size (2,1)\\nApply morphological operations for refinement\\nSet dilate_kernel to a rectangular structuring element of\\nsize (8,5)\\nApply morphological dilation to opening using di-\\nlate_kernel\\nFind contours on dilate with RETR_EXTERNAL and\\nCHAIN_APPROX_SIMPLE modes\\nif len(cnts) == 2 then\\nSet cnts to cnts[0]\\nend if\\nSet i to 4\\nfor each contour c in cnts do\\nSet rect to the bounding rectangle of c\\nif rect[2] < 20 OR rect[3] < 20 then\\ncontinue\\nend if\\nSet x, y, w, h to rect\\nWrite the subimage of binaryImage that corresponds to\\nrect to a file named str(i) + \".jpg\"\\nIncrement i by 1\\nend for\\n3) Image Segmentation\\nAfter the image preprocessing stage, the contour technique\\nis utilized to identify distinct shapes or patterns within the\\nprocessed image. This involves detecting these shapes, out-\\nlining their boundaries using bounding boxes, and potentially\\nrecognizing characters or text segments. Subsequently, these\\nsegmented areas are subjected to further analysis like charac-\\nter identification.\\nOnce the character is detected each character is cropped\\nindividually and converted into 28*28 pixels. Figure 4 and 5\\nshows the detection of Tulu numerical and characters.\\n4) Feature Extraction\\nPrinciple Component Analysis is applied on each individual\\ncropped image. In this process 784 feature of individual\\nimage is fed into PCA since images are in 28*28 pixel size.\\nPCA considers 0.98 information from the each image hence\\n291 features are considered from the image. Once feature\\nextraction is completed through the use of eigen vectors, the\\nlabels are encoded using the one-hot encoding technique.\\nThis is done to facilitate their input into machine learning\\nalgorithms for subsequent processing.\\nVOLUME , 2023 3\\nThis article has been accepted for publication in IEEE Access. This is the author\\'s version which has not been fully edited and \\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3355470\\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c3b28705-7056-439b-a594-4d6ef5aea1e3', embedding=None, metadata={'page_label': '4', 'file_name': 'Tulu_Language_Text_Recognition_and_Translation.pdf', 'file_path': 'data/Tulu_Language_Text_Recognition_and_Translation.pdf', 'file_type': 'application/pdf', 'file_size': 1427571, 'creation_date': '2025-02-28', 'last_modified_date': '2025-02-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"FIGURE 1. Block diagram of Tulu Character Recognition.\\nFIGURE 2. Different Augmented images\\nFIGURE 4. Tulu Numericals detection\\n5) Classification Using Machine Learning Algorithms\\nIn order to train our method on a dataset, we utilized\\npreparation with Convolutional Neural Network to train on\\nbureaucracy data. Multi-class categorization with a total of\\n62 classes, comprising 50 characters and 12 numerical.\\nFIGURE 5. Tulu character Detection\\nAlgorithm 2PCA for Dimensionality Reduction\\n1) Set PCA to keep 98% of the variance:\\n2) If variance threshold is not provided then\\n3) Set variance threshold to 0.98\\n4) Initialize pca with variance threshold\\n5) Fit the PCA model with the training data and transform\\n6) xtrain = Apply fit_transform method of pca with\\nx_train as input\\n7) xtest = Apply transform method of pca with x_test as\\ninput\\n8) Print the shape of xtrain and xtest\\n9) Print the number of principal components selected by\\npca\\n10) Print the number of features in the original dataset\\n4 VOLUME , 2023\\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3355470\\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f73ba7e4-f0b2-44e7-a78c-21f22a76e193', embedding=None, metadata={'page_label': '5', 'file_name': 'Tulu_Language_Text_Recognition_and_Translation.pdf', 'file_path': 'data/Tulu_Language_Text_Recognition_and_Translation.pdf', 'file_type': 'application/pdf', 'file_size': 1427571, 'creation_date': '2025-02-28', 'last_modified_date': '2025-02-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='TABLE 1. CNN Layer Hyperparameters\\nLayer Hyperparameters\\nConv2D 3 ×3 Kernel size, ReLU activation, 64 filters\\nConv2D 3 ×3 Kernel size, ReLU activation, 32 filters\\nDropout (Core Layer) 0.3 Neurons\\nMaxPool2D 2 ×2 pool size\\nDense 64 Units, ReLU activation\\nDense 62 Units, Softmax activation\\nDropout (Core Layer) 0.3 Neurons\\nConv2D 3 ×3 Kernel size, ReLU activation, 128 filters\\nMaxPool2D 2 ×2 pool size\\nIn the analysis, the convolutional neural network displays\\ngreater accuracy when contrasted with the other evaluated\\nmachine learning algorithms. Table 1 and 2 gives the detailed\\nanalysis of the CNN hyperparameters.\\nTABLE 2. Hyperparameter Values\\nHyperparameter Value\\nRegularization term kernel regularizer with L2 regularization\\nBatch size 32\\nLoss function Categorical cross-entropy\\nLearning rate 0.001\\nEpochs 150\\nOptimizer Adam\\nB. RULE BASED LANGUAGE TRANSLATION FROM\\nENGLISH TO TULU\\nThe model comprises four modules in sequence. The initial\\nmodule processes English speech input, passing its output to\\nthe subsequent module, aiming to generate Tulu text as the\\nfinal result.Figure 6 illustrates the system architecture of the\\nlanguage translator.\\n1) Preparation and Collection of Dataset\\nCollection of the dataset was done manually which includes\\nwords in english, kannada and tulu language. Though we\\nonly make use of english and tulu words in the main trans-\\nlation, we collected the corresponding kannada words which\\ncan be used in near future, for further development. The total\\nnumber of words collected initially was 1481.Training words\\nfor rule based machine translation are shown in figure 7.\\n2) Algorithm\\nEnglish text is taken as input. Spacy was utilized to perform\\nParts of Speech tagging and Named Entity Recognition on\\nthe English text, with the goal of identifying any names or\\nlocations present. If such entities were detected, they were\\ntranscribed and stored separately. The resulting modified\\nEnglish text was then translated into Kannada. For each\\nword in the Kannada text, a word-to-word translation was\\nperformed from Kannada to English, with each English word\\nbeing checked against a database. If a match was found,\\nthe corresponding Tulu word was mapped to the English\\nword. If no match was found, no translation was performed.\\nFinally, the Tulu sentence was constructed based on the\\nKannada sentence structure, with any previously identified\\nnamed entities being reintroduced. All of these steps were\\ncombined to generate the final Tulu text. Algorithm 3 shows\\nthe step by step procedure for the rule based translation.\\nAlgorithm 3Translate Text Algorithm\\nInitialize ner_dict as an empty dictionary and pos_list as\\nan empty list.\\nTokenize and tag the input text using spaCy’s nlp()\\nfunction.\\nfor each token do\\nif token has a named entity type then\\nAdd token to ner_dict.\\nelse\\nAppend (token text, POS tag) to pos_list.\\nend if\\nend for\\nTranslate the input text from English to Kannada.\\nSplit the translated text into a list of words (l).\\nfor each word in l do\\nTranslate word from Kannada to English using Trans-\\nlator.\\nend for\\nFilter out stop words and exceptions from the translated\\nwords.\\nJoin the filtered words into a single string ( string1) and\\nsplit it into a list (string).\\nOpen and read a CSV file.\\nfor each row in the CSV file do\\nfor each word in string do\\nif word is a key in ner_dict then\\nReplace the word in string with ner_dict.\\nelse\\nConstruct a regex pattern \"^\" and match it against\\nthe word.\\nif there is a match then\\nReplace the word in string with the Tulu word.\\nend if\\nend if\\nend for\\nend for\\nC. NEURAL MACHINE TRANSLATION\\nNeural Machine Translation (NMT) is an automated trans-\\nlation method that utilizes neural networks to translate text\\nfrom one language to another, replacing Statistical Machine\\nTranslation (SMT) in many applications. The fundamental\\nconcept behind NMT is to use a deep neural network to\\nlearn the correlation between the source and target language.\\nThe neural network takes the source language text as a input\\nand generates the translated text in the target language as\\nVOLUME , 2023 5\\nThis article has been accepted for publication in IEEE Access. This is the author\\'s version which has not been fully edited and \\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3355470\\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2d4a16b6-eecf-4d9e-a8d4-4d8e6b36676f', embedding=None, metadata={'page_label': '6', 'file_name': 'Tulu_Language_Text_Recognition_and_Translation.pdf', 'file_path': 'data/Tulu_Language_Text_Recognition_and_Translation.pdf', 'file_type': 'application/pdf', 'file_size': 1427571, 'creation_date': '2025-02-28', 'last_modified_date': '2025-02-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"FIGURE 6. Block diagram of Rule based Language Translation\\nFIGURE 7. Training words for rule based machine translation\\noutput. This network is trained on a large parallel corpus of\\nsentences in both languages. The process of neural machine\\ntranslation involves the computer automatically translating\\nsentences from one language to another.The steps involved\\nin neural machine translation are illustrated in Figure 7.\\nThe input layer converts the source sentence into numeri-\\ncal vectors. LSTM1 layer encodes the source sentence and\\ngenerates hidden states capturing its contextual information.\\nLSTM2 layer decodes the encoded information into the target\\nlanguage, utilizing the hidden states from LSTM1 layer.\\nThe output layer produces the final translation by computing\\nprobabilities for each target word and selecting the word with\\nthe highest probability. Figure 8 shows the structure of Neural\\nMachine Translation.\\nIn our methodology for English to Tulu neural machine\\ntranslation, we incorporated both forward and backward ap-\\nproaches. The forward approach involves training the model\\nto translate from English to Tulu, where the English sentence\\nis the input and the Tulu translation is the target. However, we\\nalso recognized the importance of the backward approach,\\nwhich involves training the model in the reverse direction,\\nfrom Tulu to English. The backward approach provides addi-\\ntional benefits by enabling the model to capture bidirectional\\nlanguage dependencies and improve the overall translation\\nquality. By training the model to translate from Tulu to\\nEnglish, we ensure that it learns to generate accurate and\\nfluent Tulu translations, leveraging the knowledge obtained\\nfrom both forward and backward training.\\n1) Data Collection and Preparation\\nTo train a neural machine translation model, we have used\\na 1000 parallel sentence of both english and tulu. Clean the\\nparallel corpus to remove noise and inconsistencies. This in-\\nvolves removing duplicate sentences, correcting typograph-\\nical errors, and normalizing punctuation, capitalization, and\\nformatting. Training sentences of neural machine translation\\nare shown in Figure 9.\\n2) Tokenization\\nThe dataset’s sentences are tokenized, meaning that each\\nsentence is converted into a sequence of integers where each\\ninteger represents a word. Separate tokenizers are utilized for\\nthe source and target languages.\\n3) Sequence Padding\\nAfter tokenization, the sequences are padded to ensure that\\nthey have the same length. This is essential since neural\\nnetworks demand input data to have the same shape. The\\nblock diagram of neural machine translation is displayed in\\nFigure 7.\\n4) Encoder Model\\nAn encoder model is built that takes the source language\\nembeddings as input and generates a fixed-size vector that\\n6 VOLUME , 2023\\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3355470\\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a2d91d54-faa3-46bc-a423-70cfa0f4344c', embedding=None, metadata={'page_label': '7', 'file_name': 'Tulu_Language_Text_Recognition_and_Translation.pdf', 'file_path': 'data/Tulu_Language_Text_Recognition_and_Translation.pdf', 'file_type': 'application/pdf', 'file_size': 1427571, 'creation_date': '2025-02-28', 'last_modified_date': '2025-02-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"FIGURE 8. Block diagram of Neural Machine Translation\\nFIGURE 9. Training sentences of neural machine translation\\nrepresents the entire input sequence. This model typically\\nuses an LSTM (Long Short-Term Memory) network, which\\nallows it to handle variable-length input sequences and cap-\\nture long-term dependencies in the input. The LSTM network\\nprocesses the input embeddings word by word and produces a\\nsequence of hidden states. The last hidden state of the LSTM\\nnetwork is then used as the fixed-size vector representation\\nof the input sequence.\\n5) Decoder Model\\nThe decoder model is designed to receive the embeddings\\nof the target language along with the output of the encoder\\nmodel as inputs, and generate the output sequence word by\\nword. Typically, an LSTM network is used in the decoder\\nmodel, which enables it to handle output sequences of vari-\\nFIGURE 10. Structure of Neural Machine Translation\\nable length and generate words based on previous words gen-\\nerated. As the target language embeddings and the encoder\\nmodel’s output are fed into the LSTM network, a sequence\\nof hidden states is produced. In each time step, the LSTM\\nnetwork of the decoder generates a probability distribution\\nover the target vocabulary using the current hidden state and\\nselects the subsequent word in the output sequence based on\\nthis distribution. Encoder decoder model is shown in Figure\\n8.\\n6) Training\\nThe training process combines training for both the encoder\\nand decoder models, using a loss function that computes the\\nvariance between the predicted and real output sequences.\\nThe LSTM network weights and other model parameters are\\noptimized using backpropagation through time. After train-\\ning, the models can be used to translate new input sequences\\nfrom the source language to the target language by encoding\\nthe input sequence using the encoder model and generating\\nthe output sequence word by word using the decoder model.\\nTable 3 describes the Hyperparameter of Encoder-Decoder\\nVOLUME , 2023 7\\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3355470\\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0ca32e8d-c377-42b1-84cf-67b568e8e26f', embedding=None, metadata={'page_label': '8', 'file_name': 'Tulu_Language_Text_Recognition_and_Translation.pdf', 'file_path': 'data/Tulu_Language_Text_Recognition_and_Translation.pdf', 'file_type': 'application/pdf', 'file_size': 1427571, 'creation_date': '2025-02-28', 'last_modified_date': '2025-02-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"FIGURE 11. Encoder Decoder Model Structure\\nModel.\\nTABLE 3. Hyperparameter of Encoder-Decoder Model\\nEncoder\\nEmbedding dimension 256\\nLSTM units 256\\nDecoder\\nEmbedding dimension 256\\nLSTM units 256\\nReturn sequences True (to return sequences at each timestep)\\nReturn state True (to return the final internal state of the LSTM)\\nTraining\\nOptimizer Adam optimizer\\nLoss function Sparse categorical cross-entropy\\nNumber of epochs 500\\nBatch size 128\\n7) Evaluation\\nTo evaluate the model’s performance on the validation set,\\nmetrics such as precision, accuracy, F1- score and recall are\\ncalculated.\\nIV. RESULTS AND DISCUSSION\\nThis section presents the experiment results on recognizing\\nhandwritten Tulu characters. In the performance analysis, the\\nCNN model achieved an accuracy of 92% with the consid-\\nered dataset. This accuracy outperforms the other algorithms,\\nas Figure 10(c) depicts. Figures 10(a) and 10(b) illustrate\\nthe accuracy and loss measures of the CNN model over 150\\nepochs.The accuracy measure is determined by analyzing\\nthe performance of the model on both the validation and\\ntraining datasets, providing insights into its generalization\\ncapabilities. Figure 10(d) depicts the performance analysis\\nTulu character recognition model by considering parameters\\nsuch as f1-score, recall, and precision. In the analysis CNN\\nmodel has better score compared to the other algorithms.\\nFIGURE 12. CNN Model Accuracy Measure\\n8 VOLUME , 2023\\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3355470\\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6f67fdcc-089d-4e17-9dce-47b2afd2710c', embedding=None, metadata={'page_label': '9', 'file_name': 'Tulu_Language_Text_Recognition_and_Translation.pdf', 'file_path': 'data/Tulu_Language_Text_Recognition_and_Translation.pdf', 'file_type': 'application/pdf', 'file_size': 1427571, 'creation_date': '2025-02-28', 'last_modified_date': '2025-02-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"FIGURE 13. CNN Model Loss Measure\\nFIGURE 14. Tulu Character Recognition Accuracy Classification\\nFIGURE 17. Performance analysis of rule based method\\nFIGURE 15. Performance Analysis of Tulu Character\\nFIGURE 16. Analysis of Blue score\\nFIGURE 18. ’a’ character detection\\nVOLUME , 2023 9\\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3355470\\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='186842de-4155-4cb7-84d3-4a410c24b37d', embedding=None, metadata={'page_label': '10', 'file_name': 'Tulu_Language_Text_Recognition_and_Translation.pdf', 'file_path': 'data/Tulu_Language_Text_Recognition_and_Translation.pdf', 'file_type': 'application/pdf', 'file_size': 1427571, 'creation_date': '2025-02-28', 'last_modified_date': '2025-02-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"FIGURE 19. ‘one’ numerical detection\\nParameters Accuracy Precision Recall f1-score\\nWord Based 89% 95% 94% 94%\\nSentence Based 81% 91% 90% 91%\\nTABLE 4. Performance analysis of rule based machine translation\\nA. CLASSIFICATION PERFORMANCE COMPARISON\\nThe performance of the classification was evaluated using\\nthe Precision, F1 Score, and Recall metrics, as defined by\\nequations (1), (2), and (3). Figure 9(d) shows the performance\\nanalysis of Tulu character recongition.\\nPrecision:\\nPrecision = TP\\nFP + TP (1)\\nRecall:\\nRecall = TP\\nTP + FN (2)\\nF1 Score:\\nF1 Score = 2 · Precision · Recall\\nPrecision + Recall (3)\\nWhere:\\n• TP = True Positive\\n• TN = True Negative\\n• FP = False Positive\\n• FN = False Negative\\nFigures 11(c) and 11(d) illustrate the process of recogniz-\\ning Tulu characters using the trained model. In this process,\\neach Tulu character is inputted into the model, and the model\\nproduces the corresponding output in the form of either\\na Kannada character or a numerical representation. Figure\\n11(b) shows the experimental result of Text-based translation\\nfrom English to Tulu model has achieved 89% accuracy on\\nword-based translation and 81% on sentence-based transla-\\ntion considered dataset. For the performance analysis, the\\nfollowing parameters were taken into consideration: accu-\\nracy, precision, recall, and F1-score. Comparing word-based\\nanalysis to sentence-based analysis, the word-based approach\\nyielded better results. Figure 11(a) depicts the variation in\\nBlue score values for English to Tulu translation over 500\\niterations, with a peak value of 0.83. For the training of\\ntranslation from English to Tulu and viceversa we have\\nconsidered single line sentence which are simple and easier\\nfor the conversion. In contrast, the Blue score values for Tulu\\nto English translation range from 0.31 to 0.65 over the same\\niteration range. Table 4 shows the performance analysis of\\nrule-based machine Translation.\\nV. CONCLUSION AND FUTURE WORK\\nThe research aimed to develop an English-to-Tulu language\\ntranslator and a literature review of existing translators for\\nIndian and other languages was conducted. Although many\\napproaches provide accurate results for words and simple\\nphrases, accuracy decreases for complex sentences. To im-\\nprove accuracy, a survey paper was written based on col-\\nlected research papers, and a dataset was manually collected\\nand tested. A handwritten character recognition system was\\ndeveloped using CNN, achieving 92% accuracy for Tulu\\ncharacters and numerals. An algorithm using a rule-based\\nmethod was incorporated into the research work for English-\\nto-Tulu translation, achieving 89% accuracy for simple words\\nand sentences. Neural machine technology was applied to\\nincrease efficiency and achieved a blue score of 0.83. The\\nmodel is currently being evaluated using individual sen-\\ntences, while the assessment of phrases is slated for future\\nconsideration. However, accuracy decreased for complex\\nsentences, indicating the need for more dataset collection\\nto improve the system. Future work includes developing a\\nreal-time application with Tulu unicode and a more complex\\nmodel and phrases can be considered in the testing of the\\nmodel.\\nVI. CONFLICT OF INTEREST\\nAuthors do not have any conflict of interest.\\nREFERENCES\\n[1] Manimozhi, I. (2021, April). An Efficient Translation of Tulu to Kannada\\nSouth Indian Scripts using Optical Character Recognition. In 2021 5th In-\\nternational Conference on Computing Methodologies and Communication\\n(ICCMC) (pp. 952-957). IEEE.\\n[2] Bhat, S., Seshikala, G. (2021). Character recognition of Tulu script using\\nconvolutional neural network. In Advances in Artificial Intelligence and\\nData Engineering (pp. 121-131). Springer, Singapore.\\n[3] Anush Bijoor, Anusha, Kripashree Bhat, Sneesha D Shetty and\\nMr.Venugopala Rao A S A CNN-Based Approach for Recognition of\\nAncient Tigalari Handwritten Characters.\\n[4] Savitha, C. K., Antony, P. J. (2018, November). Machine learning ap-\\nproaches for recognition of offline tulu handwritten scripts. In Journal of\\nPhysics: Conference Series (V ol. 1142, No. 1, p. 012005). IOP Publishing.\\n[5] Rao, A. S., Sandhya, S., Anusha, K., Arpitha, C. N., Meghana, S.\\nN. (2020). Exploring deep learning techniques for kannada handwritten\\ncharacter recognition: A boon for digitization. International Journal of\\nAdvanced Science and Technology, 29(5), 11078-11093.\\n[6] Memon, J., Sami, M., Khan, R. A., Uddin, M. (2020). Handwritten op-\\ntical character recognition (OCR): A comprehensive systematic literature\\nreview (SLR). IEEE Access, 8, 142642-142668.\\n10 VOLUME , 2023\\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3355470\\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='72a87fb9-aab1-4438-bcbd-ac6b6dd212a1', embedding=None, metadata={'page_label': '11', 'file_name': 'Tulu_Language_Text_Recognition_and_Translation.pdf', 'file_path': 'data/Tulu_Language_Text_Recognition_and_Translation.pdf', 'file_type': 'application/pdf', 'file_size': 1427571, 'creation_date': '2025-02-28', 'last_modified_date': '2025-02-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"[7] Albahli, S., Nawaz, M., Javed, A., Irtaza, A. (2021). An improved faster-\\nRCNN model for handwritten character recognition. Arabian Journal for\\nScience and Engineering, 46(9), 8509-8523.\\n[8] Bora, M. B., Daimary, D., Amitab, K., Kandar, D. (2020). Handwritten\\ncharacter recognition from images using CNN-ECOC. Procedia Computer\\nScience, 167, 2403-2409.\\n[9] Deore, S. P., Pravin, A. (2020). Devanagari handwritten character recogni-\\ntion using fine-tuned deep convolutional neural network on trivial dataset.\\nS¯adhan¯a, 45(1), 1-13.\\n[10] Khandokar, I., Hasan, M., Ernawan, F., Islam, S., Kabir, M. N. (2021,\\nJune). Handwritten character recognition using convolutional neural net-\\nwork. In Journal of Physics: Conference Series (V ol. 1918, No. 4, p.\\n042152). IOP Publishing.\\n[11] Guha, R., Das, N., Kundu, M., Nasipuri, M., Santosh, K. C. (2020).\\nDevNet: an efficient CNN architecture for handwritten Devanagari charac-\\nter recognition. International Journal of Pattern Recognition and Artificial\\nIntelligence, 34(12), 2052009.\\n[12] Rani, N. S., BR, P. (2022). Robust recognition technique for handwrit-\\nten Kannada character recognition using capsule networks. International\\nJournal of Electrical Computer Engineering (2088-8708), 12(1).\\n[13] Hamdan, Y . B. (2021). Construction of statistical SVM based recognition\\nmodel for handwritten character recognition. Journal of Information Tech-\\nnology, 3(02), 92-107.\\n[14] Vinjit, B. M., Bhojak, M. K., Kumar, S., Chalak, G. (2020, July). A\\nReview on Handwritten Character Recognition Methods and Techniques.\\nIn 2020 International Conference on Communication and Signal Processig\\n(ICCSP) (pp. 1224-1228). IEEE.\\n[15] Rani, N. S., Subramani, A. C., Kumar, A., Pushpa, B. R. (2020, July).\\nDeep learning network architecture based kannada handwritten character\\nrecognition. In 2020 Second International Conference on Inventive Re-\\nsearch in Computing Applications (ICIRCA) (pp. 213-220). IEEE.\\n[16] BJ, B. N., Athira, M. R., Prajwal, M. L. (2021, May). Kannada Confusing\\nCharacter Recognition and Classification Using Random Forest and SVM.\\nIn 2021 3rd International Conference on Signal Processing and Commu-\\nnication (ICPSC) (pp. 537-541). IEEE.\\n[17] Yadav, M., Purwar, R. K., Mittal, M. (2018). Handwritten Hindi character\\nrecognition: a review. IET Image Processing, 12(11), 1919-1933.\\n[18] Ganji, T., Velpuru, M. S., Dugyala, R. (2021). Multi variant handwritten\\ntelugu character recognition using transfer learning. In IOP Conference\\nSeries: Materials Science and Engineering (V ol. 1042, No. 1, p. 012026).\\n[19] Ayyoob, M. P., Ilyas, P. M. (2021). A review on various techniques used\\nto recognize off-line handwritten Malayalam characters. Malaya Journal\\nof Matematik (MJM), 9(1, 2021).\\n[20] Srelekha S, “Machine Translation between Malayalam and English”,\\nResearch gate 2020.\\n[21] Md. Adnanul Islam, Md. Saidul Hoque Anik and A. B. M. Alim Al Islam,\\n“An Enhanced RBMT: When RBMT Outperforms Modern Data-Driven\\nTranslators”, IETE Technical Review 2022.\\n[22] Neha Bhadwal,Prateek Agrawal, Vishu Madaan‡ “Machine Translation\\nfrom Hindi to Sanskrit”, Scalable Computing: Practice and Experience,\\n2020.\\n[23] Sitender and Seema Bawa, “A Sanskrit-to-English machine translation\\nusing hybridization Rule based”, Neural Computing and Applications\\n2021.\\n[24] Namrata Kharate, Seth Darren and Varsha Patil, “ Handling challenges in\\nrule based machine translation from marathi to english”, Research gate\\n2019.\\n[25] Kodabagi, M. M., Angadi, S. A. (2016, December). A methodology for\\nmachine translation of simple sentences from Kannada to English lan-\\nguage. In 2016 2nd International Conference on Contemporary Computing\\nand Informatics (IC3I) (pp. 237-241). IEEE.\\n[26] Salunkhe, P., Kadam, A. D., Joshi, S., Patil, S., Thakore, D., Jadhav,\\nS. (2016, March). Hybrid machine translation for English to Marathi: A\\nresearch evaluation in Machine Translation:(Hybrid translator). In 2016\\nInternational Conference on Electrical, Electronics, and Optimization\\nTechniques (ICEEOT) (pp. 924-931). IEEE.\\n[27] ARIKPO, I., DICKSON, I. (2018). Development of an automated\\nEnglish-to-local-language translator using Natural Language Processing.\\nInternational Journal of Scientific Engineering Research, 9(7), 378-383.\\n[28] Mardhotillah, R., Dirgantoro, B., Setianingsih, C. (2020, December).\\nSpeaker Recognition for Digital Forensic Audio Analysis using Support\\nVector Machine. In 2020 3rd International Seminar on Research of Infor-\\nmation Technology and Intelligent Systems (ISRITI) (pp. 514-519). IEEE.\\n[29] Dhar, P., Bisazza, A., van Noord, G. (2021, August). Optimal word\\nsegmentation for neural machine translation into Dravidian languages. In\\nProceedings of the 8th Workshop on Asian Translation (W AT2021) (pp.\\n181-190).\\n[30] Soman, K. P., Kumar, M. A., Premjith, B. (2019). Neural Machine\\nTranslation System for English to Indian Language Translation Using\\nMTIL Parallel Corpus.\\n[31] Prajapati, R., Parikh, V . V ., Majumder, P. (2021, April). Irlab-daiict@\\ndravidianlangtech-eacl2021: Neural machine translation. In Proceedings\\nof the First Workshop on Speech and Language Technologies for Dravid-\\nian Languages (pp. 262-265).\\n[32] Choudhary, H., Rao, S., Rohilla, R. (2020). Neural Machine Translation\\nfor Low-Resourced Indian Languages. arXiv preprint arXiv:2004.13819.\\n[33] Gogineni, S., Suryanarayana, G., Surendran, S. K. (2020, September). An\\nEffective Neural Machine Translation for English to Hindi Language. In\\n2020 International Conference on Smart Electronics and Communication\\n(ICOSEC) (pp. 209-214). IEEE.\\n[34] Hegde, A., Shashirekha, H. L., Madasamy, A. K., Chakravarthi, B. R.\\n(2023, March). A Study of Machine Translation Models for Kannada-\\nTulu. In Third Congress on Intelligent Systems: Proceedings of CIS 2022,\\nV olume 1 (pp. 145-161). Singapore: Springer Nature Singapore.\\n[35] Fernandes, R., Rodrigues, A. P. (2019, August). Kannada handwritten\\nscript recognition using machine learning techniques. In 2019 IEEE in-\\nternational conference on distributed computing, VLSI, electrical circuits\\nand robotics (DISCOVER) (pp. 1-6). IEEE.\\n[36] Fernandes, R., Rodrigues, A. P. (2022, December). Emotion Detection\\nin Multimedia Data Using Convolution Neural Network. In 2022 Interna-\\ntional Conference on Artificial Intelligence and Data Engineering (AIDE)\\n(pp. 157-161). IEEE.\\n[37] Dou, Z., Sun, Y ., Zhu, J., Zhou, Z. (2023). The Evaluation Prediction\\nSystem for Urban Advanced Manufacturing Development. Systems, 11(8),\\n392.\\nVOLUME , 2023 11\\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3355470\\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8ed4e7c9-275a-46c4-b415-fe78a04d2845', embedding=None, metadata={'page_label': '12', 'file_name': 'Tulu_Language_Text_Recognition_and_Translation.pdf', 'file_path': 'data/Tulu_Language_Text_Recognition_and_Translation.pdf', 'file_type': 'application/pdf', 'file_size': 1427571, 'creation_date': '2025-02-28', 'last_modified_date': '2025-02-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"PRATHWINI was born in Mangaluru, India in\\n1998. She received the B.E. degree in Informa-\\ntion Science and Engineering in 2019 and MTech\\ndegree in Computer Science and Engineering in\\n2023 from the VTU, Belagavi, India.\\nCurrently working as an Assistant Professor in\\nthe Department of Master of Computer Applica-\\ntions at NMAM Institute of Technology, Nitte,\\nIndia. She is the author of 2 research paper. Her\\nresearch interests include Natural Language Pro-\\ncessing, Machine Learning and Deep Learning.\\nANISHA P RODRIGUES was born in Man-\\ngaluru, India in 1985. She received the B.E. degree\\nin Electrical and Electronics Engineering in 2006,\\nMTech degree in Computer Science and Engineer-\\ning in 2012 and PhD degree in the area of Big Data\\nAnalytics in 2020 from the VTU, Belagavi, India.\\nSince 2012, she is working as Assistant Profes-\\nsor in the Department of Computer Science and\\nEngineering at NMAM Institute of Technology,\\nNitte, India. She is the author of 3 book chapters\\nand 20 articles. Her research interests include Natural Language Processing,\\nMachine Learning, Deep Learning and Big Data Analytics.\\nP . VIJAYA was born in Tiruchirappalli, India\\nin 1969. She received the Diploma in Electron-\\nics and Communication in 1987, B.E. degree in\\nElectronics and Communication Engineering in\\n1995, MTech degree in Computer Science and\\nEngineering in 2002 and PhD degree in the area\\nof Information Retrieval in 2017 from Karpagam\\nAcademy of Higher Education, Coimbatore, In-\\ndia.\\nCurrently she is working in Department of\\nMathematics and Computer Science, Bawshar, Muscat, Sultanate. She is the\\nauthor of 4 book chapters and 36 articles. Her research interests include\\nInformation Retrieval, Machine Learning, Deep Learning and Big Data\\nAnalytics.\\nROSHAN FERNANDES was born in Man-\\ngaluru, India in 1979. He received the B.E. de-\\ngree in Computer Science and Engineering in\\n2001 from Mangalore University, MTech degree\\nin Computer Engineering in 2007 and PhD degree\\nin the area of Mobile Web Services in 2019 from\\nthe VTU, Belagavi, India.\\nCurrently he is working as the Associate Pro-\\nfessor in the Department of Computer Science and\\nEngineering at NMAM Institute of Technology,\\nNitte, India. He has a teaching experience of 18 years in this Institution. He\\nis the author of 4 book chapters and 18 articles. His research interests include\\nMobile Web Services, Natural Language Processing, Machine Learning, and\\nDeep Learning.\\n12 VOLUME , 2023\\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3355470\\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.ingestion import IngestionPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_pipeline(documents):\n",
    "    pipeline = IngestionPipeline(transformations=transformations)\n",
    "    nodes = await pipeline.arun(documents=documents)\n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.59s/it]\n",
      "100%|██████████| 2/2 [00:08<00:00,  4.39s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.78s/it]\n",
      "100%|██████████| 1/1 [00:04<00:00,  4.72s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.96s/it]\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.29s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.46it/s]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.58s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.71it/s]\n",
      "100%|██████████| 2/2 [00:07<00:00,  3.63s/it]\n",
      "100%|██████████| 2/2 [00:05<00:00,  2.74s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.55s/it]\n",
      "100%|██████████| 15/15 [01:28<00:00,  5.88s/it]\n",
      "100%|██████████| 15/15 [02:15<00:00,  9.04s/it]\n",
      "100%|██████████| 15/15 [00:39<00:00,  2.66s/it]\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "nodes = asyncio.run(run_pipeline(documents=docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextNode(id_='43952eeb-3710-443c-a4b6-8d8455475e1f', embedding=None, metadata={'page_label': '1', 'file_name': 'Tulu_Language_Text_Recognition_and_Translation.pdf', 'file_path': 'data/Tulu_Language_Text_Recognition_and_Translation.pdf', 'file_type': 'application/pdf', 'file_size': 1427571, 'creation_date': '2025-02-28', 'last_modified_date': '2025-02-28', 'document_title': '\"Comparative Evaluation of Machine Learning Methodologies: A Comprehensive Study on Rule-Based and Neural Network Approaches for Enhancing Tulu Language Translation, Facilitating Communication Access in Karnataka\\'s Dravidian Dialects, and Its Implications for Linguistic Preservation and Technological Advancements\"\\n\\nThis title encapsulates the core elements of the document:\\n\\n- \"Comparative Evaluation\": Indicates that the study will compare rule-based and neural network techniques.\\n  \\n- \"Machine Learning Methodologies\": Specifies that machine learning approaches are under investigation.\\n\\n- \"Tulu Language Translation\": Denotes the focus on translating Tulu, a Dravidian dialect from Karnataka.\\n\\n- \"Facilitating Communication Access\": Emphasizes how improved translation can enhance communication in this community.\\n\\n- \"In Karnataka\\'s Dravidian Dialects\": Restricts the study to dialects of Tulu from Karnataka state, highlighting its regional relevance.\\n\\n- \"Implications for Linguistic Preservation and Technological Advancements\": Expands on the broader impacts, showing that this work may contribute not only to better communication but also to cultural preservation through technology and could potentially pave ways for future advancements in language technology.', 'questions_this_excerpt_can_answer': \"1. **What was the accuracy achieved by each method (rule-based and neural network) for translating English into Tulu?**\\n   - This question seeks specific, quantifiable data about the performance of two distinct translation approaches—rule-based and neural machine translation—in converting English text to the Tulu language. These results will be crucial in comparing the effectiveness of these methodologies for Tulu language translation tasks.\\n\\n2. **How do the BLEU scores (for both English to Tulu and Tulu to English translations) reflect the quality of the neural machine translation approach used?**\\n   - This question asks about a particular evaluation metric—BLEU scores—that measure the quality of machine-translated text by comparing it with reference translations. Understanding how these scores correlate with the specific encoder-decoder model employed for Tulu to English and English to Tulu translations provides insight into the performance and effectiveness of this neural network translation system tailored for the Tulu language.\\n\\n3. **In what ways did the researchers employ hybrid machine translation techniques, as mentioned in the context, to potentially enhance Tulu-to-English translation?**\\n   - This question probes into the specifics of the hybrid approach used by the authors to improve their machine translation system from English to Tulu. Understanding how combining rule-based and neural network techniques can enhance overall performance is valuable for understanding advanced strategies in language technology and could potentially be applied to other language pairs or machine translation systems.\\n\\n**Higher-level summary:**\\nThis context discusses a research paper focused on enhancing the Tulu language, a Dravidian dialect from Karnataka, for better communication access by comparing rule-based and neural network techniques in machine translation. The broader implications of such work extend beyond mere translation to include cultural preservation through technology and potential advancements in language technologies.\\n\\n**Generating questions:**\\n1. How does the proposed hybrid approach contribute to bridging gaps between different linguistic frameworks?\\n   - This question, based on a higher-level summary, queries how combining rule-based and neural machine translation methods can help maintain the richness of Tulu while improving its integration with other languages or facilitate broader linguistic understanding.\\n\\n2. In what ways could advancements in Tulu language technology from this research influence efforts to include it in India’s Constitution's 8th Schedule?\\n   - This question, inspired by the context, explores how improved translation and related technologies might support the recognition and preservation of lesser-known languages like Tulu on a larger political level.\\n\\n3. How does this study inform future research directions in developing robust machine translation systems for minority or underrepresented languages, given the success with Tulu?\\n   - This question looks at the potential impact of findings from this work to inspire further studies focused on improving translation for other endangered or less-studied languages.\", 'section_summary': \"**Key Topics:**\\n1. **Language Identification**: The primary focus is on Tulu, a Dravidian language predominantly spoken in Karnataka's southern regions. It includes four dialects essential for inter-community communication and regional trade/entertainment.\\n2. **Research Context**: This study was conducted to translate English into the Tulu language to facilitate better communication within this community, especially considering ongoing efforts to include Tulu in India’s Constitution's 8th Schedule.\\n3. **Methodologies**:\\n   - **Rule-based Machine Translation (RBMT)**: An early approach that uses predefined linguistic rules for translation, dividing sentences into components and applying these rules to generate target language output.\\n   - **Neural Network Translation with Encoder-Decoder Model using LSTM (Long Short-Term Memory)**: A more advanced method employing Convolutional Neural Networks (CNN) to process input text in English and produce high-quality Tulu translations based on large parallel datasets.\\n4. **Evaluation Metrics**:\\n   - Accuracy rates for word-based analysis and sentence-based analysis were recorded by the rule-based translation approach, with 89% and 81% respectively when translating from English to Tulu.\\n   - BLEU scores (0.83) indicate the quality of neural machine translations, demonstrating strong performance in preserving meaning and structure during the conversion process.\\n5. **Hybrid Translation Techniques**: The paper also explores combining rule-based and neural network approaches to improve overall translation performance for Tulu-to-English tasks.\\n6. **Research Team**: Comprised of scholars from NMAM Institute of Technology, NITTE (Deemed to be University), India; Modern College of Business and Sciences in Bowshar, Oman; and Department of Computer Science and Engineering at the same institution in Karnataka.\\n7. **Corresponding Author**: Roshan Fernandes, who led the research work on improving Tulu-to-English translation using machine learning methodologies.\\n8. **Publication Details**: The paper was submitted for publication in IEEE Access (Volume 2023), with potential DOI (10.1109/ACCESS.2024.3355470). \\n\\n**Entities:**\\n- Tulu: A Dravidian language primarily spoken in Karnataka’s southern regions, with four dialects used for inter-community communication and regional trade/entertainment.\\n- English: The source language for translation into Tulu to enhance communication within the Tulu community.\\n- Rule-based Machine Translation (RBMT): An early approach to language translation utilizing predefined linguistic rules.\\n- Neural Network Translation with Encoder-Decoder Model using LSTM: A contemporary method leveraging CNNs for advanced text conversion from English to Tulu and vice versa.\\n- BLEU scores: Evaluation metrics used to gauge the quality of machine translations, representing mean reciprocal rank.\\n- Research Team (Prathwini Devadiga, Anisha P Rodrigues, Vijaya, Roshan Fernandes): Scholars contributing to this study on Tulu language translation.\\n- Corresponding Author: Roshan Fernandes responsible for leading the research and writing the article.\", 'excerpt_keywords': \"Tulu Language Translation, Rule-based Machine Translation (RBMT), Neural Network Translation with Encoder-Decoder Model using LSTM, Convolutional Neural Networks (CNN), Hybrid Translation Techniques, Karnataka's Dravidian Dialects, Indian Constitution, 8th Schedule.\"}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='31c3dde4-1c46-4d38-b9df-0bea18b85d4c', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '1', 'file_name': 'Tulu_Language_Text_Recognition_and_Translation.pdf', 'file_path': 'data/Tulu_Language_Text_Recognition_and_Translation.pdf', 'file_type': 'application/pdf', 'file_size': 1427571, 'creation_date': '2025-02-28', 'last_modified_date': '2025-02-28'}, hash='dd4ee1cc8a3dad94cd389c590669355c6ec5ed6fdacf77f4d8ce09bb03432785')}, metadata_template='{key}: {value}', metadata_separator='\\n', text=\"Tulu Language Text Recognition and\\nTranslation\\nPRATHWINI1, ANISHA P RODRIGUES2, P. VIJAYA3, ROSHAN FERNANDES4*\\n1Department of Master of Computer Applications, NMAM Institute of Technology, NITTE(Deemed to be University), India\\n(e-mail:prathwini.devadiga@nitte.edu.in)\\n2Department of Computer Science and Engineering, NMAM Institute of Technology, NITTE(Deemed to be University), India (e-mail:anishapr@nitte.edu.in)\\n3Department of Mathematics and Computer Science, Modern College of Business and Sciences, Bowshar, Sultanate of Oman (e-mail:pvvijaya@gmail.com)\\n4Department of Computer Science and Engineering, NMAM Institute of Technology, NITTE(Deemed to be University), India\\n(e-mail:roshan_nmamit@nitte.edu.in)\\nCorresponding author: Roshan Fernandes (e-mail: roshan_nmamit@nitte.edu.in)\\nABSTRACT Language is a primary means of communication, but it is not the only means; knowing a\\nlanguage does, however, assist speed up the process. Many distinct languages are spoken worldwide, and\\npeople use them to communicate. This is only one of the many reasons why language is so crucial. Based\\non the literature survey, it is evident that there is a lack of available translators for the Tulu language.\\nDespite being prevalent predominantly in Karnataka, the Tulu language has not been as widely spoken as\\nother Indian languages until recently, although it gained enough recognition to become the second language\\nin Karnataka. The purpose of our research work aims at translating the English language into the Tulu\\nlanguage. During the evaluation the system was tested on a dataset consisting of handwritten characters\\nduring the evaluation process Convolutional Neural Networks used achieved an accuracy rate of 92%. To\\ntranslate English to the Tulu language, we employed a parallel sentence dataset for the neural approach and\\na parallel word dataset for the rule-based approach. The rule-based approach resulted in an 89% accuracy\\nrate for word-based analysis and an 81% accuracy rate for sentence-based analysis for the English-to-Tulu\\nlanguage translation. The neural machine translation approach of the Encoder-Decoder model with LSTM\\nis been used to accomplish translation from English to Tulu with a BLEU score of 0.83 and Tulu to English\\nwith a BLUE score of 0.65. The model also employed hybrid machine translation to enhance the translation.\\nINDEX TERMS Machine Translation, Rule-based method, Neural Network Translation, Convolutional\\nNeural Network (CNN), Encoder-Decoder Model, and Long short-term memory (LSTM)\\nI. INTRODUCTION\\nLanguage, as a means of communication, encompasses a\\ncollection of written symbols and sounds used by individ-\\nuals within a specific region or country for oral or written\\nexpression. It sets humans apart and involves acquiring a\\ncomplex framework of vocabulary, structure, and syntax for\\neffective communication. Indian languages are categorized\\ninto families such as Indo-Iranian or Indo-European, Munda,\\nDravidian, Austroasiatic, Sino-Tibetan, and Tibeto-Burman.\\nThe Indian constitution mentions twenty-two of these lan-\\nguages. India’s multilingual nature is evident across its states,\\nalthough fluency in every language spoken within the nation\\nisn’t universal. Tulu, from the Dravidian family, is spoken\\npredominantly in the southern region of Dakshina Kannada\\nand Udupi districts in Karnataka, and in Kasaragod district\\nin southwestern India. Tuluva, the indigenous people, reside\\nin Tulu Nadu. Presently, Tulu is formally acknowledged as\\nthe second language of Karnataka, with ongoing efforts to\\ninclude it in the 8th Schedule of the Constitution. It contains\\nfour dialects, mainly used for inter-community communica-\\ntion, trade, and entertainment. Tulu is spoken across various\\nregions like Mangalore, Udupi, Karkala, Belthangady, Kun-\\ndapura, Kasaragod, Manjeshwar, Puttur Sullia, and Bantwal,\\nwith different dialects in each area. Efforts are underway to\\ninclude Tulu in the Constitution’s 8th Schedule. Machine\\ntranslation (MT), a crucial aspect of natural language pro-\\ncessing, has evolved significantly. Rule-based machine trans-\\nlation (RBMT) is an early approach to language translation\\nusing predefined linguistic rules. These systems analyze in-\\nput sentences, break them down into grammatical parts, and\\nthen apply linguistic rules to generate the translated output in\\na target language. RBMT operates in three phases: analyzing\\nthe input sentence, transferring the linguistic elements into\\nthe target language, and finally, generating the translated sen-\\nVOLUME , 2023 1\\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3355470\\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\", mimetype='text/plain', start_char_idx=0, end_char_idx=4938, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, Settings\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.node_parser import SimpleNodeParser\n",
    "from llama_index.core.schema import TextNode\n",
    "from llama_index.llms.ollama import Ollama\n",
    "import asyncio\n",
    "\n",
    "# System prompt for LLM to decide response type\n",
    "system_prompt = \"\"\"\n",
    "You are an AI assistant capable of both general conversation and retrieving information from a specific PDF document.\n",
    "If a query requires information from the PDF, retrieve and return relevant details from the indexed document.\n",
    "Otherwise, respond as a general AI assistant.\n",
    "\"\"\"\n",
    "\n",
    "# Create an index from extracted nodes\n",
    "def create_index_from_nodes(nodes):\n",
    "    index = VectorStoreIndex(nodes)\n",
    "    return index\n",
    "\n",
    "# Function to generate a response where the LLM decides the response type\n",
    "async def generate_response(query, index=None):\n",
    "    prompt = f\"{system_prompt}\\nUser Query: {query}\\n\"\n",
    "    if index:\n",
    "        retriever = index.as_retriever()\n",
    "        query_engine = RetrieverQueryEngine(retriever=retriever)\n",
    "        retrieved_info = await query_engine.aquery(query)\n",
    "        prompt += f\"\\nRetrieved Information:\\n{retrieved_info.response}\"\n",
    "    return await Settings.llm.acomplete(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.24s/it]\n",
      "100%|██████████| 2/2 [00:06<00:00,  3.41s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.39s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.44it/s]\n",
      "100%|██████████| 1/1 [00:07<00:00,  7.14s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.03s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.53it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.55it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.31it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.11it/s]\n",
      "100%|██████████| 2/2 [00:03<00:00,  1.67s/it]\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.44s/it]\n",
      "100%|██████████| 15/15 [01:36<00:00,  6.42s/it]\n",
      "100%|██████████| 15/15 [02:12<00:00,  8.85s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Usage example (Assuming nodes are preprocessed)\n",
    "nodes = asyncio.run(run_pipeline(documents=docs))  # Extracted metadata nodes\n",
    "index = create_index_from_nodes(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the information from Volume 2023's study \"Comparative Analysis and Performance Evaluation of Deep Learning Models vs. Rule-Based Methods for Tulu Language's 'a' Character Recognition and Correlated Blue Score Assessment,\" here's how CNN compares to rule-based translation:\n",
      "\n",
      "1. **Character Recognition Accuracy**: The Convolutional Neural Network (CNN) model surpasses traditional rule-based translations with an accuracy of 92% on the validation set, significantly higher than other algorithms. This indicates that deep learning models like CNN are more effective at recognizing 'a' characters in this context.\n",
      "\n",
      "2. **Performance Metrics**: The CNN outperforms rule-based methods in terms of f1-score (harmonic mean of precision and recall). F1-scores around 90% demonstrate that the CNN offers superior, consistent accuracy across different character classifications compared to rule-based translations.\n",
      "\n",
      "3. **Impact on 'Blue Score'**: The Blue score, a measure of quality or reliability for this recognition task, influences both deep learning models and conventional rule-based methods (refer to FIGURE 16 in the study). CNN performs well within these constraints, emphasizing its ability to maintain high standards despite varying conditions.\n",
      "\n",
      "Thus, when considering Tulu language character recognition and translation for 'a' characters, especially taking into account performance metrics like accuracy and f1-score along with influence on Blue score, deep learning models like the CNN demonstrate superiority over rule-based translations.\n"
     ]
    }
   ],
   "source": [
    "# Sample user query\n",
    "user_query = \"How does CNN compare to rule-based translation in the PDF?\"\n",
    "response = asyncio.run(generate_response(user_query, index))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm an AI and don't have feelings, but I'm here to engage in conversation and provide information as needed. Let me tell you a story now:\n",
      "\n",
      "In the verdant hills of Karnataka, where ancient Dravidian scripts thrived, there was a team of dedicated scholars who took on the mission to preserve an important language - Tulu. Their hearts were filled with reverence for this native tongue, threatened by dominant languages like Kannada.\n",
      "\n",
      "These scholars crafted a state-of-the-art machine learning model, a deep convolutional neural network (CNN), as their eyes into the world of Tulu script. This wasn't an ordinary algorithm; it was meticulously trained with 30,500 handwritten Tulu characters. This digital sentinel could perceive and decipher Tulu text remarkably like humans.\n",
      "\n",
      "One crisp morning, they tested this creation to see how well it could translate complexities of the Tulu script into a language that could be understood globally - English or vice versa. They were delighted as their model not only translated words but also captured the subtle nuances and context of Tulu, maintaining these elements in its digital understanding.\n",
      "\n",
      "Beyond this robust CNN, they employed other machine learning tools: Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks. These were vital for capturing the intricate word structures and sentence patterns of Tulu, ensuring every detail was translated with precision.\n",
      "\n",
      "Their approach wasn't just about technology; it celebrated linguistic heritage they sought to safeguard. It harmoniously blended traditional rule-based systems with deep learning's adaptability, creating a unified framework that honored both the age of ancient scripts and modern innovation.\n",
      "\n",
      "In their digital domain of algorithms and language maps, these scholars ensured Tulu storytelling continued, one character at a time.\n"
     ]
    }
   ],
   "source": [
    "user_query = \"How are you? can you tell me a story please...\"\n",
    "response = asyncio.run(generate_response(user_query, index))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am Granite, an AI assistant developed by IBM. I specialize in understanding and explaining complex topics based on the data provided to me. In this context, we're discussing machine learning models designed for deciphering unique scripts like Tulu, with a focus on improving translation outcomes and detecting emotions from historical documents.\n"
     ]
    }
   ],
   "source": [
    "user_query = \"Who are you?\"\n",
    "response = asyncio.run(generate_response(user_query, index))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The authors of this PDF are:\n",
      "\n",
      "1. Manimozhi\n",
      "2. Seshikala et al.\n",
      "3. Anush Bijoor\n",
      "4. Savitha (or a team)\n",
      "5. Rao et al.\n",
      "6. Memon et al.\n",
      "7. Albahli\n",
      "8. Bora et al.\n",
      "9. Deore et al.\n",
      "10. Khandokar et al.\n",
      "11. Guha et al.\n",
      "12. Hamdan et al. (possibly repeated)\n",
      "13. Vinjit et al.\n",
      "14. Athira\n",
      "15. Yadav et al.\n",
      "\n",
      "(Note: There is some repetition in the list of authors, possibly due to multiple authors contributing to various sections or aspects within this document.)\n"
     ]
    }
   ],
   "source": [
    "user_query = \"who are the authors of the pdf attched?\"\n",
    "response = asyncio.run(generate_response(user_query, index))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.59s/it]\n",
      "\n",
      "\u001b[A\n",
      "100%|██████████| 2/2 [00:14<00:00,  7.02s/it]\n",
      "100%|██████████| 1/1 [00:14<00:00, 14.91s/it]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:16<00:00,  8.42s/it]\n",
      "100%|██████████| 1/1 [00:14<00:00, 14.14s/it]\n",
      "\n",
      "100%|██████████| 1/1 [00:12<00:00, 12.47s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.77s/it]\n",
      "100%|██████████| 3/3 [00:31<00:00, 10.56s/it]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:31<00:00, 31.94s/it]\n",
      "100%|██████████| 3/3 [00:44<00:00, 14.93s/it]\n",
      "100%|██████████| 1/1 [00:40<00:00, 40.93s/it]\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.21s/it]\n",
      "100%|██████████| 1/1 [00:08<00:00,  8.84s/it]\n",
      "100%|██████████| 2/2 [00:10<00:00,  5.32s/it]\n",
      "100%|██████████| 6/6 [01:12<00:00, 12.04s/it]\n",
      "100%|██████████| 6/6 [01:18<00:00, 13.06s/it]\n",
      "100%|██████████| 6/6 [01:40<00:00, 16.67s/it]\n",
      "100%|██████████| 6/6 [01:44<00:00, 17.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response 1: Based on the information from the PDF:\n",
      "\n",
      "**Comparison of CNN and Rule-Based Translation:**\n",
      "\n",
      "1. **Architecture**:\n",
      "   - CNN layers have varying configurations, including filter numbers, kernel sizes, dropout rates, max pooling operations, and dense layers with specific activation functions (ReLU or Softmax). These characteristics enable more nuanced understanding of language structures compared to rule-based systems, which typically rely on predefined rules for translation.\n",
      "\n",
      "2. **Hyperparameters**:\n",
      "   - For the rule-based system: Batch size = 32, Loss function = Categorical Cross-Entropy, Learning rate = 0.001, Epochs = 150, and Optimizer = Adam. These hyperparameters were likely chosen to optimize the performance of this neural network component within a language translation context but are not as flexible for handling diverse linguistic nuances as CNN does through its deep learning architecture.\n",
      "\n",
      "3. **Rule-Based Translation System**:\n",
      "   - This system functions in four sequential modules: input preparation, Named Entity Recognition using Spacy (for POS tagging), translation from English to Kannada and back, and finally Tulu sentence construction based on Kannada structure while preserving previously tagged named entities. The human intervention for manual handling of English language components like named entities is a notable distinction.\n",
      "\n",
      "4. **Strengths of CNN**:\n",
      "   - Automatic feature extraction from raw text data allows CNN to capture intricate linguistic patterns and nuances effectively, often outperforming rule-based systems that depend on explicit programming of each step due to their adaptability and ability to learn complex word and phrase relationships within sentences.\n",
      "\n",
      "5. **Limitations of Rule-Based System**:\n",
      "   - The reliance on manually designed grammar for translation between languages can become cumbersome as linguistic complexity increases or if new language patterns emerge. This highlights the distinct advantage CNN holds, which is its inherent adaptability and capacity to learn from data—a significant benefit in dynamic linguistic contexts.\n",
      "\n",
      "In conclusion, while rule-based translation systems excel at managing known language translations through predefined rules, deep learning models like CNN generally exhibit superior performance due to their automated feature extraction and capacity for handling complex linguistic patterns. This comparison suggests that Neural Machine Translation (NMT) with its neural network approach may offer further advantages over traditional rule-based methods by potentially learning more abstract relationships between languages in a data-driven manner.\n",
      "Response 2: I'm an artificial intelligence, so I don't have feelings or emotions; I can only provide information and respond as best I can. Now, let me tell you a story inspired by your query:\n",
      "\n",
      "In the heart of Western Ghats, where ancient Tulu script was revered in manuscripts, lived Dr. Jyothi. She was an expert in computer vision and machine learning, leading a dedicated team to preserve this rich cultural legacy digitally.\n",
      "\n",
      "Their mission: To translate centuries-old Tulu manuscripts into digital formats using handwritten character recognition. This wasn't just about creating copies; it was about ensuring these age-old stories wouldn't fade away with time. The challenge? Tulu's small script demanded meticulous preparation and cutting-edge tools to capture its unique complexities.\n",
      "\n",
      "Dr. Jyothi and her team started by compiling an extensive dataset of 30,500 carefully transcribed Tulu characters—each one a testament to the script's richness and peculiarities. \n",
      "\n",
      "They then experimented with various algorithms for recognition. Traditional rule-based systems, programmed with deep linguistic knowledge about Tulu, worked alongside neural machine translation (NMT), renowned for efficient text translation between languages. NMT proved particularly effective due to the complexities of Tulu's language structure.\n",
      "\n",
      "Among their tools was a Convolutional Neural Network (CNN), known for its ability to derive hierarchical features directly from images, making it exceptionally effective in deciphering these ancient handwriting components. To strengthen Tulu-to-English translations, they developed a hybrid approach integrating rule-based systems and NMT, leveraging each for handling specific linguistic nuances.\n",
      "\n",
      "Beyond recognition, the team crafted an Encoder-Decoder model combined with Long Short-Term Memory (LSTM). This powerful system enabled them to reverse translate—convert Tulu sentences back into English. It was a remarkable feat considering the distinct nature of this minority script.\n",
      "\n",
      "Dr. Jyothi and her crew merged traditional wisdom with technological prowess, ensuring that precious Tulu knowledge wouldn’t be lost to time. Through their work, they not only advanced computational methods for recognizing handwritten scripts but also built a bridge between cultural heritage and contemporary technology—showcasing how AI can serve as an enduring guardian of human history.\n",
      "Response 3: I am an AI assistant designed for both general conversation and retrieval of specific information from a PDF document. When your query requires information indexed in the PDF, I can provide relevant details. However, as a general AI, my primary role is to understand and generate human-like responses based on the input data provided or prompted by you.\n",
      "Response 4: The authors of the PDF attached likely comprise an interdisciplinary group with expertise in Natural Language Processing (NLP), Computational Linguistics, Computer Vision, Historical Linguistics, and specialists working on ancient scripts such as Devanagari or Dravidian. To find the precise list of authors, please open the PDF and check for acknowledgments at the document's beginning, where this information is usually listed.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.core import Settings\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Initialize LlamaIndex settings\n",
    "Settings.llm = Ollama(model=\"granite3.2:2b\", request_timeout=600)\n",
    "Settings.embed_model = OllamaEmbedding(model_name=\"nomic-embed-text\")\n",
    "\n",
    "# Define extractors and text splitter\n",
    "from llama_index.core.extractors import (\n",
    "    SummaryExtractor,\n",
    "    QuestionsAnsweredExtractor,\n",
    "    TitleExtractor,\n",
    ")\n",
    "# Uncomment if needed: from llama_index.extractors.entity import EntityExtractor\n",
    "from llama_index.core.node_parser import TokenTextSplitter\n",
    "\n",
    "test_splitter = TokenTextSplitter(separator=' ', chunk_size=1200, chunk_overlap=100)\n",
    "extractors = [\n",
    "    TitleExtractor(nodes=5),\n",
    "    QuestionsAnsweredExtractor(questions=4),\n",
    "    SummaryExtractor(summaries=[\"self\"]),\n",
    "    # KeywordExtractor(),\n",
    "    # EntityExtractor(),\n",
    "    # BaseExtractor()\n",
    "]\n",
    "transformations = [test_splitter] + extractors\n",
    "\n",
    "# Read documents from PDF\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "docs = SimpleDirectoryReader(input_files=['./data/Tulu_Language_Text_Recognition_and_Translation.pdf']).load_data()\n",
    "\n",
    "# Batch processing: create nodes and build the vector index in one step\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core import VectorStoreIndex\n",
    "import asyncio\n",
    "\n",
    "async def create_index(documents, batch_size=5):\n",
    "    pipeline = IngestionPipeline(transformations=transformations)\n",
    "    # Divide documents into batches\n",
    "    batches = [documents[i:i+batch_size] for i in range(0, len(documents), batch_size)]\n",
    "    # Process batches concurrently\n",
    "    tasks = [pipeline.arun(documents=batch) for batch in batches]\n",
    "    batch_nodes = await asyncio.gather(*tasks)\n",
    "    # Flatten the list of lists into a single list of nodes\n",
    "    all_nodes = [node for batch in batch_nodes for node in batch]\n",
    "    index = VectorStoreIndex(all_nodes)\n",
    "    return index\n",
    "\n",
    "# System prompt for LLM to decide the response type\n",
    "system_prompt = \"\"\"\n",
    "You are an AI assistant capable of both general conversation and retrieving information from a specific PDF document.\n",
    "If a query requires information from the PDF, retrieve and return relevant details from the indexed document.\n",
    "Otherwise, respond as a general AI assistant.\n",
    "\"\"\"\n",
    "\n",
    "# Function to generate a response using the system prompt and, if available, retrieved PDF info\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.schema import TextNode\n",
    "\n",
    "async def generate_response(query, index=None):\n",
    "    prompt = f\"{system_prompt}\\nUser Query: {query}\\n\"\n",
    "    if index:\n",
    "        retriever = index.as_retriever()\n",
    "        query_engine = RetrieverQueryEngine(retriever=retriever)\n",
    "        retrieved_info = await query_engine.aquery(query)\n",
    "        prompt += f\"\\nRetrieved Information:\\n{retrieved_info.response}\"\n",
    "        \n",
    "    return await Settings.llm.acomplete(prompt)\n",
    "\n",
    "# Usage example: create the index and generate responses\n",
    "index = asyncio.run(create_index(docs, batch_size=5))\n",
    "\n",
    "# Example queries:\n",
    "user_query = \"How does CNN compare to rule-based translation in the PDF?\"\n",
    "response = asyncio.run(generate_response(user_query, index))\n",
    "print(\"Response 1:\", response)\n",
    "\n",
    "user_query = \"How are you? Can you tell me a story please...\"\n",
    "response = asyncio.run(generate_response(user_query, index))\n",
    "print(\"Response 2:\", response)\n",
    "\n",
    "user_query = \"Who are you?\"\n",
    "response = asyncio.run(generate_response(user_query, index))\n",
    "print(\"Response 3:\", response)\n",
    "\n",
    "user_query = \"Who are the authors of the pdf attached?\"\n",
    "response = asyncio.run(generate_response(user_query, index))\n",
    "print(\"Response 4:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
